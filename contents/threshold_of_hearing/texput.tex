% Emacs, this is -*-latex-*-

\title{Perceptual Coding Considering the Threshold of Hearing}

\maketitle
\tableofcontents

\section{A model of the Threshold of (Human) Hearing}

Psychoacoustics (see
\href{https://vicente-gonzalez-ruiz.github.io/the_sound/}{the sound},
\href{https://vicente-gonzalez-ruiz.github.io/human_auditory_system/}{the
  human auditory system}, and
\href{https://vicente-gonzalez-ruiz.github.io/human_sound_perception/}{the
  human sound perception}) has determined that the HAS (Human Auditory
System) has a sensitivity that depends on the frequency of the sound,
the so called ToH
(\href{https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing}{Threshold
  of (Human) Hearing}). This basically means that some subbands (intervals of frequencies)can be
quantized with a larger quantization step than others without a
noticeable increase (from a perceptual perspective) of the
quantization noise~\cite{sayood2017introduction}.

\begin{figure}
  \centering
  %\svg{graphics/ToHH}{1800}
  \myfig{graphics/ToHH}{47cm}{1400}
  \caption{A model for the threshold of human hearing.}
  \label{fig:ToHH}
\end{figure}

A good approximation of ToH for a 20-year-old person can be
obtained with~\cite{bosi2003intro}
\begin{equation}
  T(f)\text{[dB]} = 3.64(f\text{[kHz]})^{-0.8} - 6.5e^{f\text{[kHz]}-3.3)^2} + 10^{-3}(f\text{[kHz]})^4.
  \label{eq:ToHH}
\end{equation}
This equation has been plotted in Fig.~\ref{fig:ToHH}.

\section{DWT subbands and quantization steps (basic algorithm)}
The number of DWT subbands
\begin{equation}
  N_{\text{sb}} = N_{\text{levels}} + 1
\end{equation}
where $N_{\text{levels}}$ is the number of levels of the
DWT~\cite{vetterli1995wavelets}. Except for the
${\mathbf l}^{N_{\text{levels}}}$ subband (the lowest-pass frequency
of the decomposition), it holds that
\begin{equation}
  W({\mathbf w}_s) = \frac{1}{2}W({\mathbf w}_{s-1}),
\end{equation}
being $W(\cdot)$ the bandwidth of the corresponding
subband. Therefore, considering that the bandwidth of the audio signal
is $22050$ Hz, the bandwidth $W({\mathbf w}_1)=11025$ Hz,
$W({\mathbf w}_2)=22025/4$, etc. It also holds that
\begin{equation}
  W({\mathbf l}^{N_{\text{levels}}}) = W({\mathbf w}^{N_{\text{levels}}}).
\end{equation}

The idea is to decide, knowing the frequencies represented in each DWT
subband and the ToH curve (see
\href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver}), the QSS (Quantization Step Size) that should
be applied to each subband.

This idea is already implemented in a module named \verb|basic_ToH.py|.

\section{A higher frequency-resolution approach}
\label{sec:more_subbands}

The frequency resolution of a dyadic subband partition generated by
the DWT could not be high enough to map the ToH curve
accurately.\footnote{For example, if $N_{\text{levels}=5}$ we
  decompose the audio into 6 subbands, and the Bark scale has 24
  subbands. Remember that when the
  \href{https://en.wikipedia.org/wiki/Wavelet_transform}{Wavalet
    transform} is
  \href{https://en.wikipedia.org/wiki/Dyadic_rational}{dyadic}, the
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{Wavelet
    space} is analyzed by
  \href{https://en.wikipedia.org/wiki/Octave_band}{octaves}, and
  therefore the
  \href{https://en.wikipedia.org/wiki/Filter_bank}{subbands} doubles
  their size when we increase the frequency (see \href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver}). Anyway, the higher the numbe of subbands, the better the
  approximation to the ToH curve.} To overcome this, we can use a
decomposition with more subbands and a good tool could be
\href{https://en.wikipedia.org/wiki/Wavelet_packet_decomposition}{Wavelet
  Packets}. Notice that PyWavelets provides an
\href{https://pywavelets.readthedocs.io/en/latest/ref/wavelet-packets.html}{implementation}.

\begin{comment}
will filter each DWT subband
considering the corresponding part of the ToH curve. To achieve this,
we will use the
\href{https://numpy.org/doc/stable/reference/routines.fft.html}{FFT
  (Fast Fourier Transform)} to map each DWT subband to the Fourier
domain and filter the signal using the ToH curve without generating a
significant increase or decrease in the signal energy. To do this, we
should quantize and dequantize each FFT subband, using the
corresponding QSS.

Finally, notice that (before using the FFT) a temporal window
(different from the square window, which is the one we are using if
we don't apply a
\href{https://en.wikipedia.org/wiki/Window_function}{windowing
  technique}) should be used to minimize the
\href{https://en.wikipedia.org/wiki/Spectral_leakage}{spectral
  leakage}.
\end{comment}

So far, this technique has not been implemented.

\section{Customizable ToH curves}
\label{sec:better_ToH}

The ToH plotted in Fig.~\ref{fig:ToHH} can be different to your
current ``perceptual hearing capabilities''.\footnote{For example,
  your speakers could not have a flat frequency response, or your room
  could attenuate some frequencies.} An optimal-user-specific ToH
should take into consideration your noticeable
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization
  noise} in each subband, defining a set of QSSs (one per subband). To
find such set, the following algorithm can be used:

\begin{enumerate}
\item %Let
  %$\{{\mathbf l}^{N_{\text{levels}}}, {\mathbf h}^{N_{\text{levels}}},
  %{\mathbf h}^{N_{\text{levels}}-1},\cdots, {\mathbf h}^1\}$ the
  %wavelet representation of a
  %chunk. %, being ${\mathbf l}^{N_{\text{levels}}}$ the lowest frequency subband.
  Starting with the lowest frequency subband (at the first
  iteration the rest of subbands are zero). While the noise
  (suppose that the quantization noise follows an
  \href{https://en.wikipedia.org/wiki/Continuous_uniform_distribution}{uniform
    distribution}) is imperceptible:
  \begin{enumerate}
  \item Increase the amplitude of the noise in the subband.
  \item Compute the inverse transform, generating a chunk of audio.
  \item Reproduce the generated chunk, alternating every second with
    a silence (the play of an empty chunk).
  \end{enumerate}
\item Continue with the next subband, but keeping the
  highest unperceptible noise in the previously processed
  ones.
\end{enumerate}

Notice that the QSSs are determined for the sound that you are going
to play (not for the audio that you are generating). Therefore, you
should use your interlocutor's QSSs and vice versa. Implement also the
transmission of this information.

Finally, this improvement should be optional through a command line
parameter (the ``standard'' ToH of the Fig.~\ref{fig:ToHH}, should be
used by default).

%\section{What you have to do?}

%\subsection{Determine your THH}

\begin{comment}
\subsection{Subjective performance}

\begin{enumerate}
\item Using a recording tool such as
  \href{http://audacity.sourceforge.net}{Audacity} or
  \href{http://plugin.org.uk/timemachine/}{JACK Timemachine}, record
  the simulated transmission of a piece of audio and create a
  \texttt{.wav} file, when the audio has been transmitted using
  \texttt{temporal\_overlapped\_DWT\_coding.py} and
  \texttt{threshold.py}, using in both cases the same transmission
  bit-rate. Vary the quantization step size for controlling the
  bit-rate.
\item Determine which audio sounds better from a subjective point of
  view. Repeat this step the number of times you consider necessary.
\end{enumerate}
\end{comment}

\section{Deliverables}

Implement the functionality described in
Sections~\ref{sec:more_subbands} and \ref{sec:better_ToH} in a module
\verb|advanced_ToH.py|, and a report showing how your proposal works,
including a subjective performance comparison.

\section{Resources}

\bibliography{maths,data_compression,DWT,audio_coding}

