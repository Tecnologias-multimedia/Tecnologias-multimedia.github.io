% Emacs, this is -*-latex-*-

%\title{Perceptual Coding Considering the Threshold of Hearing}
\title{Perceptual Quantization}

\maketitle
\tableofcontents

\section{A model of the Threshold of (Human) Hearing}

Psychoacoustics (see
\href{https://vicente-gonzalez-ruiz.github.io/the_sound/}{the sound},
\href{https://vicente-gonzalez-ruiz.github.io/human_auditory_system/}{the
  human auditory system}, and
\href{https://vicente-gonzalez-ruiz.github.io/human_sound_perception/}{the
  human sound perception}) has determined that the HAS (Human Auditory
System) has a sensitivity that depends on the frequency of the
sound. Such behaviour is described by the so called ToH
(\href{https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing}{Threshold
  of (Human) Hearing}). This basically means that some subbands
(intervals of frequencies) can be quantized with a larger quantization
step than others without a noticeable increase (from a perceptual
perspective) of the quantization noise~\cite{sayood2017introduction}.

\begin{figure}
  \centering
  %\svg{graphics/ToHH}{1800}
  \myfig{graphics/ToHH}{47cm}{1400}
  \caption{A model for the threshold of human hearing.}
  \label{fig:ToHH}
\end{figure}

A good approximation of ToH for a 20-year-old person can be
obtained with~\cite{bosi2003intro}
\begin{equation}
  T(f)\text{[dB]} = 3.64(f\text{[kHz]})^{-0.8} - 6.5e^{f\text{[kHz]}-3.3)^2} + 10^{-3}(f\text{[kHz]})^4.
  \label{eq:ToHH}
\end{equation}
This equation has been plotted in Fig.~\ref{fig:ToHH}.

\section{Bark Scale (BS)}

The frequency resolution of the HAS is finite. This basically means
that two tonal sounds almost sound the same when they have similar
frequencies. Moreover, the minimal distance in frequency to confuse
both depends on their frequencies: if the frequency is low, the
distance must be smaller. Such behaviour can be described by the
\href{https://en.wikipedia.org/wiki/Bark_scale}{Bark scale} (see also
\href{https://ccrma.stanford.edu/~jos/bbt/Bark_Frequency_Scale.html}{this}),
were as it can be seen, the size of the ``critical'' bands increases
with the frequency.

%\section{Perceptual quantization}

Considering both concepts, the ToH and the BS, we can improve
(subjectively) the quality of the sound for a given bit-rate. The idea
is to use a different QSS for each critical band. The QSSs should
resemble the ToH curve, and the bandwidth of the subbands should
follow the tendence of the size of the critical bands.

\section{DWT's dyadic decomposition}

The number of subbands generated by the DWT is
\begin{equation}
  N_{\text{DWT}} = L_{\text{DWT}}+1,
\end{equation}
where $L_\text{DWT}$ is the number of levels of the
DWT~\cite{vetterli1995wavelets}. Notice that, except for the
${\mathbf l}^{N_{\text{levels}}}$ subband (the lowest-pass frequency
of the decomposition), it holds that
\begin{equation}
  W({\mathbf w}_s) = \frac{1}{2}W({\mathbf w}_{s-1}),
\end{equation}
being $W(\cdot)$ the bandwidth of the corresponding
subband. Therefore, considering that (by default, in InterCom) the
bandwidth of the audio signal is $22050$ Hz, the bandwidth $W({\mathbf
  w}_1)=22050/2$ Hz, $W({\mathbf w}_2)=22050/4$, etc. It is also true
that (see
\href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver})
\begin{equation}
  W({\mathbf l}^{L_{\text{DWT}}}) = W({\mathbf w}^{L_{\text{DWT}}}).
\end{equation}

Unfortunately, as it can be seen, the DWT does not provide a good
decomposition if we want to use a different QSS for each critical band
($N_{\text{DWT}}$ is generally too small\footnote{$N_{\text{DWT}}$
depends also on the chunk-size, a value that should be small enough to
minimize the latency.} and the size of the subbandas does not resemble
the BS critical bands).

\section{Wavelet Packet Transform (WPT)}

The WPT is an extensiÃ³n of the DWT where the 2-channels PRFB is
applied also recursively to the high frequencies (see (see Milestone
\href{https://tecnologias-multimedia.github.io/contents/transform_coding/}{\emph{Transform
  Coding for Redundancy Removal}}).}) Now, the number of subbands
  genearted by the WPT is
\begin{equation}
  N_{\text{WPT}} = 2^{L_{\text{WPT}}},
\end{equation}
where $L_\text{DWT}$ is the number of levels of the
DWT~\cite{vetterli1995wavelets}.

Unfortunately (again), although in this case, $N_{\text{WPT}}$ can be
much larger than $N_{\text{DWT}}$,
\begin{equation}
  W({\mathbf w}_s) = W({\mathbf w}_{s-1})\quad \forall s,
\end{equation}
i.e., all the WPT subbands have the same bandwidth which neither it is
the most suitable to mimic the BS critical bands.

\section{Hybrid dyadic-linear decomposition}

A better frequency partition can be found computing first the DWT
(dyadic decomposition) and then the WPT (linear decomposition) of each
dyadic subband:
\begin{enumerate}
\item For each input chunk $c$:
  \begin{enumerate}
  \item Compute the DWT of each channel of $c$, generating the decomposition $\{l^{N_{\text{DWT}}}, h^{N_{\text{DWT}}}, h^{N_{\text{DWT}}-1},\cdots,h^{1}\}$.
  \item for each dyadic subband $\mathbf{w}$ in $\{l^{N_{\text{DWT}}}, h^{N_{\text{DWT}}}, h^{N_{\text{DWT}}-1},\cdots,h^{1}\}$:
    \begin{enumerate}
      \item Compute the WPT of $\mathbf{w}$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}
Notice that the number of resulting subbands generated by the hybrid
DWT+WPT transform is
\begin{equation}
  N_{\text{DWT+WPT}} = 2^{L_{\text{WPT}}}(L_{\text{DWT}}+1).
\end{equation}

This algorithm has been implemented in
\texttt{perceptual_quantization.py} which inputs two new parameters:
\texttt{--levels_DWT} and \texttt{--levels_WPT}. Notice that to avoid
distortion at the chunk boundaries we apply the hybrid transform
DWT+WPT on extended chunks that obviously overlap.

\section{Not everyone hears the same}

The ToH curve~\cite{bosi2003intro} varies between individuals:
\begin{enumerate}
\item In general, women hear better than men.
\item With age, we lose sensitivity to high frequencies.
\item Exposure to loud noises over time can \emph{elevate} the ToH,
  especially in individuals exposed to loud sound.
\item Auditory training can help to detect sounds at lower intensities
  or distinguish subtle nuances in tone.
  % \item Short-duration sounds may require higher intensities to be
  %   perceived compared to longer-duration ones. -> a un nuevo
  %   hito. https://en.wikipedia.org/wiki/Critical_band
\end{enumerate}

And this can be said without considering your local audio
infraestructure.\footnote{For example,
  your speakers could not have a flat frequency response, or your room
  could attenuate some frequencies.}

Therefore, it is quite improbable that, at least in practice, you
have a ToH identical to the theoretical curve shown in the previous
milestone.

\section{Custom ToH curves}

If you are talking to a semi-deaf interlocutor, why would you send
them auditory information that they will not be able to perceive? A
similar question arises when our audio equipments are not very
good. Therefore, a suitable user-specific ToH curve should take into
consideration the user's \textbf{noticeable}
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization
  noise} in each DWT+WPT subband.\footnote{Remember that when we
quantize the coefficients of a subband we are basically generating
quantization noise that can be modeles as random
\href{https://en.wikipedia.org/wiki/Continuous_uniform_distribution}{uniform
  (uniformely distributed)} noise.} Thus, we can estimate the energy
of the noticeable quantizaiton noise by generating,
subband-by-subband, random uniform numbers of increasing amplitude as
the coefficients of the subband, and playing the resulting inverse
transform. The minimum noticeable ``energy value'' of each subband is
written in a file named \texttt{custom\_ToH\_curve.txt}. Notice that
this file should be used by your interlocutor, and viceversa.

\section{Deliverables}
\begin{enumerate}
\item Find your own ToH curve using \texttt{create\_ToH\_curve.py}. In
  this case we are going to transmit the sound file through
  \texttt{localhost}, and therefore, we don't have to send the file.
\item For the same bit-rate (approximately\footnote{It is quite
difficult to use exacly the same bit-rate.}), compare\footnote{Since
we are doing a subjective comparison we cannot use a standard R/D
curve.}  the quality of the reproduction of an audio file when you use
  \texttt{static\_ToH.py} and its predecessor,
  \texttt{temporal\_DWT.py}.
\end{enumerate}

%%%%%%%%%%

\section{Dyadic DWT subbands and quantization steps}
The number of dyadic DWT subbands
\begin{equation}
  N_{\text{sb}} = N_{\text{levels}} + 1,
\end{equation}
where $N_{\text{levels}}$ is the number of levels of the
dyadic DWT~\cite{vetterli1995wavelets}. Except for the
${\mathbf l}^{N_{\text{levels}}}$ subband (the lowest-pass frequency
of the decomposition), it holds that
\begin{equation}
  W({\mathbf w}_s) = \frac{1}{2}W({\mathbf w}_{s-1}),
\end{equation}
being $W(\cdot)$ the bandwidth of the corresponding
subband. Therefore, considering that (by default, in InterCom) the
bandwidth of the audio signal is $22050$ Hz, the bandwidth $W({\mathbf
  w}_1)=22050/2$ Hz, $W({\mathbf w}_2)=22050/4$, etc. It is also true
that
\begin{equation}
  W({\mathbf l}^{N_{\text{levels}}}) = W({\mathbf w}^{N_{\text{levels}}}).
\end{equation}

The idea is to decide, knowing the frequencies represented in each DWT
subband and the ToH curve (see
\href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver}), the QSS (Quantization Step Size) that should
be applied to each subband.

This idea is already implemented in a module named \verb|dyadic_ToH.py|.

\section{Deliverables}

Subjectively compare the audio quality obtained by
\verb|dyadic_ToH.py| and its predecessor,
\verb|temporal_overlapped_DWT_coding.py|. ``Subjectively'' means that, in
groups, you must determine, for the same bit-rate and audio-content
configuration, which implementation sounds better.

%Mark: \textbf{1 point}.

\section{Resources}

\bibliography{maths,data_compression,DWT,audio_coding}

