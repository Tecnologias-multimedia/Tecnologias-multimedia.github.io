% Emacs, this is -*-latex-*-

\title{Perceptual Coding Considering the Threshold of Hearing}

\maketitle

\section{Description}

\subsection{A model of the Threshold of (Human) Hearing}

Psychoacoustics (see
\href{https://vicente-gonzalez-ruiz.github.io/the_sound/}{the sound},
\href{https://vicente-gonzalez-ruiz.github.io/human_auditory_system/}{the
  human auditory system}, and
\href{https://vicente-gonzalez-ruiz.github.io/human_sound_perception/}{the
  human sound perception}) has determined that the HAS (Human Auditory
System) has a sensitivity that depends on the frequency of the sound,
the so called ToH
(\href{https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing}{Threshold
  of (Human) Hearing}). This basically means that some subbands can be
quantized with a larger quantization step than others without a
noticeable increase (from a perceptual perspective) of the
quantization noise~\cite{sayood2017introduction}.

\begin{figure}
  \centering
  %\svg{graphics/ToHH}{1800}
  \myfig{graphics/ToHH}{47cm}{1400}
  \caption{A model for the threshold of human hearing.}
  \label{fig:ToHH}
\end{figure}

A good approximation of the ToH for a 20-years old person can be
obtained with~\cite{bosi2003intro}
\begin{equation}
  T(f)\text{[dB]} = 3.64(f\text{[kHz]})^{-0.8} - 6.5e^{f\text{[kHz]}-3.3)^2} + 10^{-3}(f\text{[kHz]})^4.
  \label{eq:ToHH}
\end{equation}
This equation has been plotted in the Fig.~\ref{fig:ToHH}.

\subsection{DWT subbands and quantization steps (basic algorithm)}
The number of DWT subbands
\begin{equation}
  N_{\text{sb}} = N_{\text{levels}} + 1
\end{equation}
where $N_{\text{levels}}$ is the number of levels of the
DWT~\cite{vetterli1995wavelets}. Except for the
${\mathbf l}^{N_{\text{levels}}}$ subband (the lowest-pass frequency
of the decomposition), it holds that
\begin{equation}
  W({\mathbf h}^s) = \frac{1}{2}W({\mathbf h}^{s-1}),
\end{equation}
being $W(\cdot)$ the bandwidth of the corresponding subband
$s$. Therefore, considering that the bandwidth of the audio signal is
$22050$ Hz, the bandwidth $W({\mathbf h}^1)=11025$ Hz,
$W({\mathbf h} ^2)=22025/4$, and so on. It also holds that
\begin{equation}
  W({\mathbf l}^{N_{\text{levels}}}) = W({\mathbf h}^{N_{\text{levels}}}).
\end{equation}

The idea is to decide, knowing the frequencies represented in each DWT
subband and the ToH curve (see
\href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver}), the QSS (Quantization Step Size) that should
be applied to each subband. This idea has been implemented in the
module \verb|basic_ToH.py|.

\section{A higher frequency-resolution approach}
\label{sec:FFT}

The frequency resolution of the dyadic subband partition generated by
the DWT could not be high enough to map the ToH curve accurately. To
overcome this, we will filter each DWT subband considering the
corresponding part of the ToH curve. To achieve this, we will use the
\href{https://numpy.org/doc/stable/reference/routines.fft.html}{FFT
  (Fast Fourier Transform)} to map each DWT subband to the Fourier
domain and filter the signal using the ToH curve without generating a
significative increment or decrement of the energy of the signal. To
do this, we should quantize and dequantize each FFT subband, using the
corresponding QSS.

Finally, notice that (before to the use the FFT) a temporal window
(different to the square window, which is the one that we are using if
we don't apply a
\href{https://en.wikipedia.org/wiki/Window_function}{windowing
  technique}) should be used to minimize the
\href{https://en.wikipedia.org/wiki/Spectral_leakage}{spectral
  leakage}.

This technique has not been implemented, yet.

\section{Better ToHs}
\label{sec:better_ToH}

The ToH plotted in the Fig.~\ref{fig:ToHH} can be
different to the curve which corresponds with your current ``hearing
capabilities''.\footnote{For example, your speakers could not have a
  flat frequency response, or your room could attenuate more, some
  freqencies.} For this reason, in the module \verb|advanced_ToH.py|,
implement a procedure for determining the QSSs used in
each Fourier subband\footnote{Remember that when the
  \href{https://en.wikipedia.org/wiki/Wavelet_transform}{Wavalet
    transform} is
  \href{https://en.wikipedia.org/wiki/Dyadic_rational}{dyadic}, the
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{Wavelet
    space} is analyzed by
  \href{https://en.wikipedia.org/wiki/Octave_band}{octaves}, and
  therefore the
  \href{https://en.wikipedia.org/wiki/Filter_bank}{subbands} doubles
  the size when we increase the frequency (see \href{https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb}{
  InterCom: a Real-Time Digital Audio Full-Duplex
  Transmitter/Receiver}).}.
  
To find out such QSSs, we can simulate
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization
  noise} in each FFT subband and measure the amplitude of the noise when
it starts to be noticeable. Supposing that the quantization noise
follows an
\href{https://en.wikipedia.org/wiki/Continuous_uniform_distribution}{uniform
  distribution}, the next algorithm can be implemented:

\begin{enumerate}
\item Let
  $\{{\mathbf l}^{N_{\text{levels}}}, {\mathbf h}^{N_{\text{levels}}},
  {\mathbf h}^{N_{\text{levels}}-1},\cdots, {\mathbf h}^1\}$ the
  Wavelet representation of a
  chunk. %, being ${\mathbf l}^{N_{\text{levels}}}$ the lowest frequency subband.
  Starting with ${\mathbf l}^{N_{\text{levels}}}$ (at the first
  iteration the rest of subbands are zero), split the wavelet subband
  into a (configurable) number of Fourier subbands. While the noise stays
  imperceptible:
  \begin{enumerate}
  \item Increase the amplitude of the noise in the Fourier subband.
  \item Compute the inverse FFT/Wavelet-transform.
  \item Reproduce the generated chunk, alternating every second with
    the play of an empty chunk.
  \end{enumerate}
\item Continue with the next Fourier/Wavelet subband, but keeping the
  highest unperceptible uniform noise in the previously processed
  ones. In this way, for the wavelet subband ${\mathbf h}^1$ (the last
  one to be processed), the rest of subbands should be already
  noisy, and the same holds for the Fourier subbands that
  belong to each wavelet subband.
\end{enumerate}

Notice that the QSSs are determined for the sound that you are going
to play (not for the audio that you are generating). Therefore, you
should use your interlocutor's QSSs and viceversa. Implement also the
transmission of this information.

Finally, the determination of the ToH should be optional for the users of
InterCom (the ``standard'' ToH, see Fig.~\ref{fig:ToHH}, should be
used by default).

\section{What you have to do?}

%\subsection{Determine your THH}
Implement the functionality described in Sections~\ref{sec:FFT} and
\ref{sec:better_ToH}.

\begin{comment}
\subsection{Subjective performance}

\begin{enumerate}
\item Using a recording tool such as
  \href{http://audacity.sourceforge.net}{Audacity} or
  \href{http://plugin.org.uk/timemachine/}{JACK Timemachine}, record
  the simulated transmission of a piece of audio and create a
  \texttt{.wav} file, when the audio has been transmitted using
  \texttt{temporal\_overlapped\_DWT\_coding.py} and
  \texttt{threshold.py}, using in both cases the same transmission
  bit-rate. Vary the quantization step size for controlling the
  bit-rate.
\item Determine which audio sounds better, from a subjective point of
  view. Repeat this step the number of times you consider necessary.
\end{enumerate}
\end{comment}

\section{Deliverables}

The improved module \verb|advanced_ToH.py| and a report of how your proposal works,
including a subjective performance comparison.

\section{Resources}

\bibliography{maths,data_compression,DWT,audio_coding}

