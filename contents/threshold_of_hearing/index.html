<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Perceptual Coding Considering the Threshold of Hearing</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Perceptual Coding Considering the Threshold of
Hearing</h2>
 <div class='author'><span class='ecrm-1200'>Vicente González Ruiz</span></div><br />
<div class='date'><span class='ecrm-1200'>March 30, 2023</span></div>
   </div>
   <h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-10001'></a>Description</h3>
<!-- l. 9 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>1.1   </span> <a id='x1-20001.1'></a>A model of the Threshold of (Human) Hearing</h4>
<!-- l. 11 --><p class='noindent'>Psychoacoustics (see <a href='https://vicente-gonzalez-ruiz.github.io/the_sound/'>the sound</a>, <a href='https://vicente-gonzalez-ruiz.github.io/human_auditory_system/'>the human auditory system</a>, and <a href='https://vicente-gonzalez-ruiz.github.io/human_sound_perception/'>the human sound
perception</a>) has determined that the HAS (Human Auditory System) has a
sensitivity that depends on the frequency of the sound, the so called ToH (<a href='https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing'>Threshold
of (Human) Hearing</a>). This basically means that some subbands can be quantized
with a larger quantization step than others without a noticeable increase (from a
perceptual perspective) of the quantization noise <span class='cite'>[<a href='#Xsayood2017introduction'>2</a>]</span>.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 28 --><p class='noindent'><div style='text-align:center;'> <img src='graphics/ToHH.svg' /> </div>  <a id='x1-2001r1'></a>
<a id='x1-2002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>A model for the threshold of human hearing.                    </span></figcaption><!-- tex4ht:label?: x1-2001r1  -->
                                                                  

                                                                  
   </figure>
<!-- l. 33 --><p class='indent'>   A good approximation of ToH for a 20-year-old person can be obtained with <span class='cite'>[<a href='#Xbosi2003intro'>1</a>]</span>
\begin {equation}  T(f)\text {[dB]} = 3.64(f\text {[kHz]})^{-0.8} - 6.5e^{f\text {[kHz]}-3.3)^2} + 10^{-3}(f\text {[kHz]})^4. \label {eq:ToHH}  \end {equation}
This equation has been plotted in Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a>.
</p>
   <h4 class='subsectionHead'><span class='titlemark'>1.2   </span> <a id='x1-30001.2'></a>DWT subbands and quantization steps (basic algorithm)</h4>
<!-- l. 42 --><p class='noindent'>The number of DWT subbands \begin {equation}  N_{\text {sb}} = N_{\text {levels}} + 1  \end {equation}
where \(N_{\text {levels}}\) is the number of levels of the DWT <span class='cite'>[<a href='#Xvetterli1995wavelets'>3</a>]</span>. Except for the \({\mathbf l}^{N_{\text {levels}}}\) subband (the
lowest-pass frequency of the decomposition), it holds that \begin {equation}  W({\mathbf h}^s) = \frac {1}{2}W({\mathbf h}^{s-1}),  \end {equation}
being \(W(\cdot )\) the bandwidth of the corresponding subband \(s\). Therefore, considering that the
bandwidth of the audio signal is \(22050\) Hz, the bandwidth \(W({\mathbf h}^1)=11025\) Hz, \(W({\mathbf h} ^2)=22025/4\), etc. It also holds that \begin {equation}  W({\mathbf l}^{N_{\text {levels}}}) = W({\mathbf h}^{N_{\text {levels}}}).  \end {equation}
</p><!-- l. 61 --><p class='indent'>   The idea is to decide, knowing the frequencies represented in each DWT subband
and the ToH curve (see <a href='https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb'>InterCom: a Real-Time Digital Audio Full-Duplex
Transmitter/Receiver</a>), the QSS (Quantization Step Size) that should be
applied to each subband. This idea has been implemented in the module
<span class='obeylines-h'><span class='verb'><span class='ectt-1000'>basic_ToH.py</span></span></span>.
</p><!-- l. 69 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-40002'></a>A higher frequency-resolution approach</h3>
<!-- l. 72 --><p class='noindent'>The frequency resolution of the dyadic subband partition generated by the DWT
could not be high enough to map the ToH curve accurately. To overcome this, we will
filter each DWT subband considering the corresponding part of the ToH curve. To
achieve this, we will use the <a href='https://numpy.org/doc/stable/reference/routines.fft.html'>FFT (Fast Fourier Transform)</a> to map each DWT
subband to the Fourier domain and filter the signal using the ToH curve without
generating a significant increase or decrease in signal energy. To do this, we
should quantize and dequantize each FFT subband, using the corresponding
QSS.
</p><!-- l. 83 --><p class='indent'>   Finally, notice that (before using the FFT) a temporal window (different from the
square window, which is the one we are using if we don’t apply a <a href='https://en.wikipedia.org/wiki/Window_function'>windowing
technique</a>) should be used to minimize the <a href='https://en.wikipedia.org/wiki/Spectral_leakage'>spectral leakage</a>.
</p><!-- l. 91 --><p class='indent'>   This technique has not yet been implemented.
</p><!-- l. 93 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-50003'></a>Better ToHs</h3>
<!-- l. 96 --><p class='noindent'>The ToH plotted in Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a> can be different from the curve that corresponds to your current “hearing
capabilities”.<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-5001f1'></a> For
                                                                  

                                                                  
this reason, in module <span class='obeylines-h'><span class='verb'><span class='ectt-1000'>advanced_ToH.py</span></span></span>, implement a procedure to determe the QSSs used in each
Fourier subband<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-5002f2'></a>.
</p><!-- l. 115 --><p class='indent'>   To find such QSSs, we can simulate <a href='https://en.wikipedia.org/wiki/Quantization_(signal_processing)'>quantization noise</a> in each FFT subband and
measure the amplitude of the noise when it starts to be noticeable. Supposing that
the quantization noise follows an <a href='https://en.wikipedia.org/wiki/Continuous_uniform_distribution'>uniform distribution</a>, the next algorithm can be
implemented:
</p><!-- l. 123 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5004x1'>
     <!-- l. 124 --><p class='noindent'>Let \(\{{\mathbf l}^{N_{\text {levels}}}, {\mathbf h}^{N_{\text {levels}}}, {\mathbf h}^{N_{\text {levels}}-1},\cdots , {\mathbf h}^1\}\) the Wavelet representation of a chunk. Starting with \({\mathbf l}^{N_{\text {levels}}}\) (at the first
     iteration the rest of subbands are zero), split the wavelet subband into
     a (configurable) number of Fourier subbands. While the noise stays
     imperceptible:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-5006x1'>Increase the amplitude of the noise in the Fourier subband.
         </li>
<li class='enumerate' id='x1-5008x2'>Compute the inverse FFT/Wavelet-transform.
         </li>
<li class='enumerate' id='x1-5010x3'>Reproduce the generated chunk, alternating every second with the
         play of an empty chunk.</li></ol>
     </li>
<li class='enumerate' id='x1-5012x2'>Continue with the next Fourier/Wavelet subband, but keeping the highest
     unperceptible uniform noise in the previously processed ones. In this way, for
     the wavelet subband \({\mathbf h}^1\) (the last one to be processed), the rest of subbands
     should be already noisy, and the same holds for the Fourier subbands that
     belong to each wavelet subband.</li></ol>
<!-- l. 147 --><p class='indent'>   Notice that the QSSs are determined for the sound that you are going to play
(not for the audio that you are generating). Therefore, you should use your
interlocutor’s QSSs and vice versa. Implement also the transmission of this
information.
</p><!-- l. 152 --><p class='indent'>   Finally, the determination of the ToH should be optional for the users of
InterCom (the “standard” ToH, see Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a>, should be used by default).
                                                                  

                                                                  
</p><!-- l. 156 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4   </span> <a id='x1-60004'></a>What you have to do?</h3>
<!-- l. 159 --><p class='noindent'>Implement the functionality described in Sections <a href='#x1-40002'>2<!-- tex4ht:ref: sec:FFT  --></a> and <a href='#x1-50003'>3<!-- tex4ht:ref: sec:better_ToH  --></a>.
</p><!-- l. 180 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5   </span> <a id='x1-70005'></a>Deliverables</h3>
<!-- l. 182 --><p class='noindent'>The improved module <span class='obeylines-h'><span class='verb'><span class='ectt-1000'>advanced_ToH.py</span></span></span> and a report of how your proposal works,
including a subjective performance comparison.
</p><!-- l. 185 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>6   </span> <a id='x1-80006'></a>Resources</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xbosi2003intro'></a>M. Bosi and R.E. Goldberd.  <a href='https://last.hit.bme.hu/download/vidtechlab/fcc/literature/audio/audio_coding_standards_book.pdf'><span class='ecti-1000'>Introduction to Digital Audio Coding and
   </span><span class='ecti-1000'>Standards</span></a>. Kluwer Academic Publishers, 2003.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xsayood2017introduction'></a>K. Sayood.    <a href='http://rahilshaikh.weebly.com/uploads/1/1/6/3/11635894/data_compression.pdf'><span class='ecti-1000'>Introduction  to  Data  Compression</span></a>  <a href='https://people.cs.nctu.edu.tw/~cmliu/Courses/Compression/'><span class='ecti-1000'>(Slides)</span></a>.    Morgan
   Kaufmann, 2017.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xvetterli1995wavelets'></a>M. Vetterli  and  J. Kovačević.     <a href='http://waveletsandsubbandcoding.org/Repository/VetterliKovacevic95_Manuscript.pdf'><span class='ecti-1000'>Wavelets  and  Subband  Coding</span></a>.
   Prentice-hall, 1995.
</p>
   </div>
<p><a id='Q1-1-10'></a></p>
   <div class='footnotes'><!-- l. 100 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>For example, your speakers could not have a flat frequency response, or your room could
</span><span class='ecrm-0800'>attenuate more, some freqencies.</span></p>
<!-- l. 113 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>Remember that when the </span><a href='https://en.wikipedia.org/wiki/Wavelet_transform'><span class='ecrm-0800'>Wavalet transform</span></a> <span class='ecrm-0800'>is </span><a href='https://en.wikipedia.org/wiki/Dyadic_rational'><span class='ecrm-0800'>dyadic</span></a><span class='ecrm-0800'>, the </span><a href='https://en.wikipedia.org/wiki/Discrete_wavelet_transform'><span class='ecrm-0800'>Wavelet space</span></a> <span class='ecrm-0800'>is analyzed by</span>
<a href='https://en.wikipedia.org/wiki/Octave_band'><span class='ecrm-0800'>octaves</span></a><span class='ecrm-0800'>, and therefore the </span><a href='https://en.wikipedia.org/wiki/Filter_bank'><span class='ecrm-0800'>subbands</span></a> <span class='ecrm-0800'>doubles the size when we increase the frequency (see </span><a href='https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb'><span class='ecrm-0800'>InterCom:
</span><span class='ecrm-0800'>a Real-Time Digital Audio Full-Duplex Transmitter/Receiver</span></a><span class='ecrm-0800'>).</span></p>                                       </div>
 
</body> 
</html>