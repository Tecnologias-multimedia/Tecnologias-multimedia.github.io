<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Perceptual Coding Considering the Threshold of Hearing</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Perceptual Coding Considering the Threshold of
Hearing</h2>
 <div class='author'><a href='https://vicente-gonzalez-ruiz.github.io/'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>&amp; </span><a href='https://hpca.ual.es/~savins/'><span class='ecrm-1200'>Savins Puertas Martín</span></a> <span class='ecrm-1200'>&amp; </span><a href='https://www.marcoslupion.com/'><span class='ecrm-1200'>Marcos Lupión Lorente</span></a></div><br />
<div class='date'><span class='ecrm-1200'>November 1, 2024</span></div>
   </div>
   <h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>A model of the Threshold of (Human) Hearing</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-4'>DWT subbands and quantization steps (basic algorithm)</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-5'>A higher frequency-resolution approach</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-6'>Customizable ToH curves</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-7'>Deliverables</a></span>
<br />    <span class='sectionToc'>6 <a href='#x1-70006' id='QQ2-1-8'>Resources</a></span>
   </div>
<!-- l. 8 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-20001'></a>A model of the Threshold of (Human) Hearing</h3>
<!-- l. 10 --><p class='noindent'>Psychoacoustics (see <a href='https://vicente-gonzalez-ruiz.github.io/the_sound/'>the sound</a>, <a href='https://vicente-gonzalez-ruiz.github.io/human_auditory_system/'>the human auditory system</a>, and <a href='https://vicente-gonzalez-ruiz.github.io/human_sound_perception/'>the human sound
perception</a>) has determined that the HAS (Human Auditory System) has a
sensitivity that depends on the frequency of the sound, the so called ToH (<a href='https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing'>Threshold
of (Human) Hearing</a>). This basically means that some subbands (intervals of
frequencies)can be quantized with a larger quantization step than others without
a noticeable increase (from a perceptual perspective) of the quantization
noise <span class='cite'>[<a href='#Xsayood2017introduction'>2</a>]</span>.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 27 --><p class='noindent'><div style='text-align:center;'> <img src='graphics/ToHH.svg' /> </div>  <a id='x1-2001r1'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>A model for the threshold of human hearing.                    </span></figcaption><!-- tex4ht:label?: x1-2001r1  -->
                                                                  

                                                                  
   </figure>
<!-- l. 32 --><p class='indent'>   A good approximation of ToH for a 20-year-old person can be obtained with <span class='cite'>[<a href='#Xbosi2003intro'>1</a>]</span> \begin {equation}  T(f)\text {[dB]} = 3.64(f\text {[kHz]})^{-0.8} - 6.5e^{f\text {[kHz]}-3.3)^2} + 10^{-3}(f\text {[kHz]})^4. \label {eq:ToHH}  \end {equation}<a id='x1-2002r1'></a>
This equation has been plotted in Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a>.
</p>
   <h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-30002'></a>DWT subbands and quantization steps (basic algorithm)</h3>
<!-- l. 41 --><p class='noindent'>The number of DWT subbands \begin {equation}  N_{\text {sb}} = N_{\text {levels}} + 1  \end {equation}<a id='x1-3001r2'></a> where \(N_{\text {levels}}\) is the number of levels of the DWT <span class='cite'>[<a href='#Xvetterli1995wavelets'>3</a>]</span>.
Except for the \({\mathbf l}^{N_{\text {levels}}}\) subband (the lowest-pass frequency of the decomposition), it holds
that \begin {equation}  W({\mathbf w}_s) = \frac {1}{2}W({\mathbf w}_{s-1}),  \end {equation}<a id='x1-3002r3'></a> being \(W(\cdot )\) the bandwidth of the corresponding subband. Therefore, considering that
the bandwidth of the audio signal is \(22050\) Hz, the bandwidth \(W({\mathbf w}_1)=11025\) Hz, \(W({\mathbf w}_2)=22025/4\), etc. It also holds that
\begin {equation}  W({\mathbf l}^{N_{\text {levels}}}) = W({\mathbf w}^{N_{\text {levels}}}).  \end {equation}<a id='x1-3003r4'></a>
</p><!-- l. 60 --><p class='indent'>   The idea is to decide, knowing the frequencies represented in each DWT subband
and the ToH curve (see <a href='https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb'>InterCom: a Real-Time Digital Audio Full-Duplex
Transmitter/Receiver</a>), the QSS (Quantization Step Size) that should be
applied to each subband. This idea has been implemented in the module
<span class='obeylines-h'><span class='verb'><span class='ectt-1000'>basic_ToH.py</span></span></span>.
</p><!-- l. 68 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-40003'></a>A higher frequency-resolution approach</h3>
<!-- l. 71 --><p class='noindent'>The frequency resolution of a dyadic subband partition generated
by the DWT could not be high enough to map the ToH curve
accurately.<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-4001f1'></a>
To overcome this, we can use a decomposition with more subbands and a
good tool could be <a href='https://en.wikipedia.org/wiki/Wavelet_packet_decomposition'>Wavelet Packets</a>. Notice that PyWavelets provides an
<a href='https://pywavelets.readthedocs.io/en/latest/ref/wavelet-packets.html'>implementation</a>.
</p><!-- l. 113 --><p class='indent'>   So far, this technique has not been implemented.
</p><!-- l. 115 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4   </span> <a id='x1-50004'></a>Customizable ToH curves</h3>
<!-- l. 118 --><p class='noindent'>The ToH plotted in Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a> can be different to your current “perceptual hearing
capabilities”.<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-5001f2'></a>
An optimal-user-specific ToH should take into consideration your noticeable
<a href='https://en.wikipedia.org/wiki/Quantization_(signal_processing)'>quantization noise</a> in each subband.
</p><!-- l. 127 --><p class='indent'>   For this reason, in module named <span class='obeylines-h'><span class='verb'><span class='ectt-1000'>advanced_ToH.py</span></span></span>, implement a procedure to
determe the QSSs used in each subband.
</p><!-- l. 131 --><p class='indent'>   To find such QSSs, we can simulate <a href='https://en.wikipedia.org/wiki/Quantization_(signal_processing)'>quantization noise</a> in each subband and
measure the amplitude of the noise when it starts to be noticeable. Supposing that
the quantization noise follows an <a href='https://en.wikipedia.org/wiki/Continuous_uniform_distribution'>uniform distribution</a>, the next algorithm can be
                                                                  

                                                                  
implemented:
</p><!-- l. 139 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-5004x1'>
     <!-- l. 145 --><p class='noindent'>Starting with the lowest frequency subband (at the first iteration the rest of
     subbands are zero). While the noise (suppose that the quantization noise
     follows an <a href='https://en.wikipedia.org/wiki/Continuous_uniform_distribution'>uniform distribution</a>) stays imperceptible:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-5006x1'>Increase the amplitude of the noise in the subband.
         </li>
<li class='enumerate' id='x1-5008x2'>Compute the inverse transform.
         </li>
<li class='enumerate' id='x1-5010x3'>Reproduce  the  generated  chunk,  alternating  every  second  with  a
         silence (the play of an empty chunk).</li></ol>
     </li>
<li class='enumerate' id='x1-5012x2'>Continue with the next subband, but keeping the highest unperceptible noise in
     the previously processed ones.</li></ol>
<!-- l. 161 --><p class='indent'>   Notice that the QSSs are determined for the sound that you are going to play
(not for the audio that you are generating). Therefore, you should use your
interlocutor’s QSSs and vice versa. Implement also the transmission of this
information.
</p><!-- l. 166 --><p class='indent'>   Finally, this improvement should be optional through a command line parameter
(the “standard” ToH, see Fig. <a href='#x1-2001r1'>1<!-- tex4ht:ref: fig:ToHH  --></a>, should be used by default).
</p><!-- l. 192 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5   </span> <a id='x1-60005'></a>Deliverables</h3>
<!-- l. 194 --><p class='noindent'>Implement the functionality described in Sections <a href='#x1-40003'>3<!-- tex4ht:ref: sec:FFT  --></a> and <a href='#x1-50004'>4<!-- tex4ht:ref: sec:better_ToH  --></a> in a module
<span class='obeylines-h'><span class='verb'><span class='ectt-1000'>advanced_ToH.py</span></span></span>, and a report how your proposal works, including a subjective
performance comparison.
                                                                  

                                                                  
</p><!-- l. 199 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>6   </span> <a id='x1-70006'></a>Resources</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xbosi2003intro'></a>M. Bosi and R.E. Goldberd.  <a href='https://last.hit.bme.hu/download/vidtechlab/fcc/literature/audio/audio_coding_standards_book.pdf'><span class='ecti-1000'>Introduction to Digital Audio Coding and
   </span><span class='ecti-1000'>Standards</span></a>. Kluwer Academic Publishers, 2003.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xsayood2017introduction'></a>K. Sayood.    <a href='http://rahilshaikh.weebly.com/uploads/1/1/6/3/11635894/data_compression.pdf'><span class='ecti-1000'>Introduction  to  Data  Compression</span></a>  <a href='https://people.cs.nctu.edu.tw/~cmliu/Courses/Compression/'><span class='ecti-1000'>(Slides)</span></a>.    Morgan
   Kaufmann, 2017.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xvetterli1995wavelets'></a>M. Vetterli  and  J. Kovačević.     <a href='http://waveletsandsubbandcoding.org/Repository/VetterliKovacevic95_Manuscript.pdf'><span class='ecti-1000'>Wavelets  and  Subband  Coding</span></a>.
   Prentice-hall, 1995.
</p>
   </div>
   <div class='footnotes'><a id='x1-4002x3'></a>
<!-- l. 87 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>For example, if</span> \(N_{\text {levels}=5}\) <span class='ecrm-0800'>we decompose the audio into 6 subbands, and the Bark scale has 24
</span><span class='ecrm-0800'>subbands. Remember that when the </span><a href='https://en.wikipedia.org/wiki/Wavelet_transform'><span class='ecrm-0800'>Wavalet transform</span></a> <span class='ecrm-0800'>is </span><a href='https://en.wikipedia.org/wiki/Dyadic_rational'><span class='ecrm-0800'>dyadic</span></a><span class='ecrm-0800'>, the </span><a href='https://en.wikipedia.org/wiki/Discrete_wavelet_transform'><span class='ecrm-0800'>Wavelet space</span></a> <span class='ecrm-0800'>is
</span><span class='ecrm-0800'>analyzed by </span><a href='https://en.wikipedia.org/wiki/Octave_band'><span class='ecrm-0800'>octaves</span></a><span class='ecrm-0800'>, and therefore the </span><a href='https://en.wikipedia.org/wiki/Filter_bank'><span class='ecrm-0800'>subbands</span></a> <span class='ecrm-0800'>doubles their size when we increase the
</span><span class='ecrm-0800'>frequency (see </span><a href='https://github.com/Tecnologias-multimedia/InterCom/blob/master/docs/2-hours_seminar.ipynb'><span class='ecrm-0800'>InterCom: a Real-Time Digital Audio Full-Duplex Transmitter/Receiver</span></a><span class='ecrm-0800'>).
</span><span class='ecrm-0800'>Anyway, the higher the numbe of subbands, the better the approximation to the ToH
</span><span class='ecrm-0800'>curve.</span></p><a id='x1-5002x4'></a>
<!-- l. 121 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>For example, your speakers could not have a flat frequency response, or your room could
</span><span class='ecrm-0800'>attenuate some frequencies.</span></p>                                                                                      </div>
 
</body> 
</html>