% Emacs, this is -*-latex-*-

\title{Hidding the Jitter}

\maketitle
\tableofcontents

\section{Latencies}
\href{https://en.wikipedia.org/wiki/Latency_(engineering)#Communication_latency}{Communication
  latency} (also called
\href{https://en.wikipedia.org/wiki/Network_delay}{network delay} and
\href{https://en.wikipedia.org/wiki/End-to-end_delay}{end-to-end
  delay}) is the time it takes for that a chunk of data (encapsulated
in an \href{https://en.wikipedia.org/wiki/Internet_Protocol}{IP}
\href{https://en.wikipedia.org/wiki/Network_packet}{packet} in the
case of the Internet) to travel from one point of the network to
another. This time is relevant for an intercom(municator) because the
total latency $t_u$ that users experiment can be approximated by
\begin{equation}
  t_u = t_p + t_i,
  \label{eq:user_latency}
\end{equation}
where $t_p$ is the \emph{propagation time} (or propagation latency) of
the \href{https://en.wikipedia.org/wiki/Telecommunications_link}{link}
and $t_i$ is the \emph{latency} generated by the intercom.

Due to the current design of the Internet~\cite{Tanenbaum,Stallings}
(where the available
\href{https://en.wikipedia.org/wiki/Bandwidth_(computing)}{bandwidth}
is shared on demand by the network users) $t_p$ is variable in time
and cannot be controlled without using
\href{https://en.wikipedia.org/wiki/Quality_of_service}{Quality of
  Service (QoS)}\footnote{Something that is not available to normal
network users.}~\cite{dordal2020intro}. In contrast, $t_i$ is constant
for a given intercom configuration/implementation.

\section{Using a buffer to hidden the jitter}

We can consider that the
\href{https://en.wikipedia.org/wiki/Quality_of_experience}{Quality of
  Experience (QoE)} provided by InterCom is inversely proportional to
the \href{https://en.wikipedia.org/wiki/Packet_delay_variation}{jitter
  of the network} (see Fig.~\ref{fig:timelines}-a). One solution (see
Fig.~\ref{fig:timelines}-b) is the use of a
\href{https://en.wikipedia.org/wiki/Random_access}{random access}
\href{https://en.wikipedia.org/wiki/Data_buffer}{buffer} at the
receiver side, where the chunks are stored for a time large enough to
hide the jitter~\cite{Kurose-Ross}.

\begin{figure}
  \begin{center}
    \myfig{graphics/timelines}{10cm}{1000}
  \end{center}
  \caption{Timelines of two InterCom interactions. On the left, the
    playback is defective because some chunks are lost. On the right,
    the audio rendering is correct because the playback has been
    delayed 2 chunk times (enough for this example).}
  \label{fig:timelines}
\end{figure}

\href{https://en.wikipedia.org/wiki/Jitter#Jitter_buffers}{\emph{Dejitterizing}
  buffers}\footnote{Usually implemented with random access buffers.}
are typically implemented as a circular buffer structure (see
Fig.~\ref{fig:circular_buffer}). In an ideal situation (as depicted in
the figure), the number of pending-to-be-played chunks available in
the buffer is half of the number of slots in the buffer, and the
chunks have arrived on time. In this example
(Fig.~\ref{fig:circular_buffer}), the receiver (where the chunks are
buffered) waits for 3 chunks before starting playing the chunk number
0.\footnote{Implementation tip: in a system where for each recorded
  chunk a chunk must be also played, a delay in the playback can be
  generated by sending zero-chunks to the DAC and then, after the
  delay, start sending the received chunk of audio, in the right
  order.} Notice that the number of slots in the buffer, $2N$, must
double the number of chunks buffered during the buffering time
proportional to $N$, in order to hide a jitter of $N$
chunks-time. Notice also that this technique also introduces a $N$
chunk-time delay in the playback.

\begin{figure}
  \begin{tabular}{ccc}
    \vbox{\myfig{graphics/circular_buffer1}{2cm}{200}} & \vbox{\myfig{graphics/circular_buffer2}{2cm}{200}} & \vbox{\myfig{graphics/circular_buffer3}{2cm}{200}} \\
    (a) & (b) & (c)
  \end{tabular}
  \caption{A circular buffer with 6 slots (space for 6 chunks). Half
    of the buffer is occupied (slots in gray) because the buffering
    time is 3 chunk times and 3 chunks (with chunk number 0, 1 and 2
    in the beginning, subfigure (a)) have been received. The first
    chunk to be played is chunk 0 (subfigure (b)). Then, a new chunk
    (chunk number 3) is received and buffered (subfigure (c)).}
  \label{fig:circular_buffer}
\end{figure}

For this new improved InterCom, users must provide a new parameter
called ``\emph{buffering time}''. This value (typically expressed in
milliseconds) should be large enough to hide the network jitter but
small enough to keep the end-to-end (user) latency below some limit.

The following guidelines have been used to implement the
\emph{buffered} version of InterCom:

\begin{enumerate}
\item The class Minimal has been
  \href{https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)}{inherited}
  (extended) to implement a new class Buffering.
\item In the payload of each UDP packet, a chunk number (a 16-bits
  counter) has been included in order to provide to the receiver the
  information which determine where to store the corresponding chunk in
  the circular buffer.
\item It has been taken into consideration that the critical part of
  InterCom (the method \verb|record_send_and_play()|) is a method that
  runs as an
  \href{https://en.wikipedia.org/wiki/Interrupt_handler}{interrup
    handler} that is called each time a new chunk is available in the
  \href{https://en.wikipedia.org/wiki/Analog-to-digital_converter}{ADC}. More
  precisely:
  \begin{enumerate}
  \item To minimize the latency, the recorded chunks are sent
    to the interlocutor as soon as possible.
  \item The playback of the chunks extracted from the buffer is
    \href{https://en.wikipedia.org/wiki/Gapless_playback}{\emph{gapless}}
    (without the occurrence of silences) as long as the chunks have
    been received on time.
  \item The lost\footnote{Those chunks that have not been
  received on time or never have been received.} chunks are replaced
    by zero-chunks in the playback.
  \end{enumerate}
\item A method \verb|receive_and_buffer()| runs now in a different
  execution
  \href{https://en.wikipedia.org/wiki/Thread_(computing)}{thread},
  decoupled from the \verb|record_send_and_play()| method. But notice
  that this can be achieved without using the
  \href{https://docs.python.org/3/library/threading.html}{threading}
  or
  \href{https://docs.python.org/3/library/multiprocessing.html}{multiprocessing}
  packages because the interruption handler (the so called
  \emph{callback}() function in
  \href{https://python-sounddevice.readthedocs.io/en/0.3.14/api.html}{sounddevice})
  already runs in parallel with the main thread.
\end{enumerate}

This is an overview of the implementation:

\begin{lstlisting}[language=Python]
  # Interruption handler
  def record_send_and_play():
    chunk = record()  # (1)
    packed_chunk = pack(chunk)  # (2)
    send(packed_chunk)  # (3)
    chunk = unbuffer_next_chunk()  # (4)
    play(chunk)  # (5)

  # Main (not a new) thread
  def receive_and_buffer():
    packed_chunk = receive()  # (1)
    chunk_number, chunk = unpack(packed_chunk)  # (2)
    buffer(chunk_number, chunk)  # (3)
\end{lstlisting}

Notice that Step (4) of the method \verb|record_send_and_play()|
extracts from the buffer an unpacked\footnote{Chunks will be
compressed in a posterior milestone, and uncompressing is performed in
this step.} chunk. The chunks are buffered in Step (3) of the
method \verb|receive_and_buffer()|. Note also that Step (1) of
\verb|receive_and_buffer()| is a blocking method that should return with
every new received chunk.

%\begin{pseudocode}{Buffering\_InterCom}{~}
%  \PROCEDURE{record\_send\_and\_play}{~}
%  \BEGIN
%    \text{chunk} \GETS \text{record}()\\
%    \text{packed\_chunk} \GETS \text{pack}(\text{chunk})\\
%    \text{send}(\text{packed\_chunk})\\
%    \text{chunk} \GETS \text{unbuffer\_next\_chunk}()\\
%    \text{play}(\text{chunk})
%  \END
%  \ENDPROCEDURE
%  \PROCEDURE{receive\_and\_buffer}{~}
%  \BEGIN
%    \text{packed\_chunk} \GETS \text{receive}()\\
%    \text{chunk\_number}, \text{chunk} \GETS \text{unpack}(\text{packe%d\_chunk})\\
%    \text{buffer}(\text{chunk\_number}, \text{chunk})
%  \END
%  \ENDPROCEDURE
%\end{pseudocode}

\section{What you have to do? (deliverables)}
\label{sec:homework}

In this milestone we are going to measure the QoE provided by our
``minimal'' (or ``NAT\_traversal'') intercom when the network latency
varies \href{https://en.wikipedia.org/wiki/Randomness}{at random}. At
this point, we basically have two alternatives:
\begin{enumerate}
\item Run two instances of InterCom in two different
  \href{https://en.wikipedia.org/wiki/Host_(network)}{host}s interconected
  by a communication link.
\item Locally (in the same host), run instances\footnote{In the
general case, we can use two instances but it is also possible to use
only one.} of InterCom and control the network latency.
\end{enumerate}
Each option has pros and cons, from which we can highlight that:
\begin{enumerate}
\item \textbf{Using a real link:}
  \begin{itemize}
  \item (Pro) The use of real latency is
    going to show the true behavior of InterCom between the used hosts,
    which, in general, is quite impredictable (depend basically on the
    \href{https://en.wikipedia.org/wiki/Network_congestion}{network
      congestion}).
  \item (Con) To run InterCom in two different hosts we will need to
    establish a direct communication between them and it is very
    likely that we will be in different LANs interconnected by one or
    more
    \href{https://tecnologias-multimedia.github.io/contents/NAT_traversal/}{NAT
      devices}~\cite{srisuresh1999nat}.
  \end{itemize}
\item \textbf{Using a simulated link:}
  \begin{itemize}
  \item (Pro) The simulation of the link latency allows to control the
    behavior of the link, allowing, for example, to run InterCom in
    situations that are difficult to reproduce in a real network.
  \item (Con) The running environment must provide a way of
    controlling the latency between
    \href{https://en.wikipedia.org/wiki/Process_(computing)}{processes}. Fortunately,
    in Linux\footnote{In other OS, other tools could be avaiable. If
      this is not the case, remember that we can run InterCom in a
      virtual machine to control the network latency.} we can control
    the latency (and the
    \href{https://en.wikipedia.org/wiki/Bit_rate}{bit-rate}) of the
    outgoing traffic (packets) using the command
    \href{https://man7.org/linux/man-pages/man8/tc.8.html}{\texttt{tc}}
    \cite{bert2012lartc}.
  \end{itemize}
\end{enumerate}

\subsection{Measurement}
In most cases we will test InterCom only in our host. Therefore, it
can be useful to have an idea of how the latencies are, at
least from a statistical point of view.

\begin{figure}
  \begin{center}
    \myfig{graphics/ping_timeline}{8cm}{500}
    % \svgfig{graphics/ping_timeline}{8cm}{1800}
  \end{center}
  \caption{Timeline of a ping interaction between two hosts A and B,
    interconnected by simple communication link.}
  \label{fig:ping_timeline}
\end{figure}

To measure latencies, we will use
\href{https://github.com/torvalds/linux/blob/master/net/ipv4/ping.c}{\texttt{ping}}~\cite{Kurose-Ross,Forouzan},
a tool that
\href{https://en.wikipedia.org/wiki/Ping_(networking_utility)}{sends}
(one or more)
\href{https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol}{ICMP}
Echo Request messages to an IP address and waits for receiving (one or
more) ICMP Echo Reply messages generated by the
(\href{https://en.wikipedia.org/wiki/Operating_system}{OS} of that)
host, measuring the so called
\href{https://en.wikipedia.org/wiki/Round-trip_delay}{RTT} (Round-Trip
Time). For example, in the Figure~\ref{fig:ping_timeline} are
described the different time components in which a RTT can be
decomposed. In this figure, $t_t$ stands for \emph{transmission time},
and $t_p$ (again) for \emph{propagation time}. A simple link (two
wires, for example) using
\href{https://en.wikipedia.org/wiki/Time-division_multiple_access}{TDM}
(Time-Domain Multiplexing) has been supposed. For this reason, the
propagation and transmission times are identical in both
directions. Notice that if the payload of the \verb|ping| message has
only 64 bytes (the default value in most \verb|ping| implementations)
and the bit rate of the link is high, then $t_p\gg t_t.$ In a LAN, for
example, it also holds that
\begin{equation}
  \text{RTT} = 2t_p + 2t_t.
  \label{eq:RTT}
\end{equation}

\subsection{Characterize the latency in different scenarios}

\subsubsection{In your host}

Do the following steps:

\begin{enumerate}
\item Ping \verb|localhost|:
   \begin{lstlisting}{language=bash}
ping localhost -c 100 -s <payload_length_in_bytes> > /tmp/ping.txt
  \end{lstlisting}
  Use a payload size similar to the chunk size (in bytes) that you
  expect to use in your experiments.
\item Compute the expected (considering that the RTT should
  double the) latency to the host of your interlocutor:\\\\
  \verb|export LC\_NUMERIC=en\_US.UTF-8 # Use "." instead of "," for the decimal separator|\\
\texttt{grep from < /tmp/ping.txt | cut -f 4 -d "=" | cut -f 1 -d " " | awk
  \textquotesingle\{print \$1/2\}\textquotesingle~> /tmp/localhost\_latencies.txt}\\\\

\item Find the histogram of the expected latencies:
  
  \begin{lstlisting}{language=bash}
cat << EOF | python -
import numpy as np
from scipy import stats
latencies = np.loadtxt("/tmp/localhost_latencies.txt")
average_latency = np.average(latencies)
print("\naverage latency =", average_latency)
max_latency = np.max(latencies)
min_latency = np.min(latencies)
maximum_absolute_deviation = max(max_latency - average_latency, average_latency - min_latency)
print("maximum absolute deviation (jitter) =", maximum_absolute_deviation)
correlation_coefficient = stats.pearsonr(latencies, np.roll(latencies, 1))[0]
print("Pearson correlation coefficient =", correlation_coefficient)
if correlation_coefficient < 0:
  print("Correlation coefficient < 0: use 0 (no correlation between RTT samples) in your experiments")
histogram = np.histogram(latencies)
np.savetxt("/tmp/localhost_histogram.txt", histogram[0])
EOF
  \end{lstlisting}

\item Plot the histogram:
  \begin{lstlisting}{language=bash}
gnuplot
plot "/tmp/localhost_histogram.txt" with histogram
  \end{lstlisting}
  
\item Characterize statistically the latency: Which statistical
  distribution is closer to your experimental results?
\end{enumerate}

\subsubsection{In the Internet}

This scenario can be useful to test InterCom in your host, but
simulating later a real connection between hosts in different local
networks\footnote{If you are unable to use two different local
  networks, try to run InterCom in two hosts connected to the same
  router.}. For doing that:

\begin{enumerate}
  
\item Repeat\footnote{Remember that the data files must be processed
    to extract the latencies.} the previous experiment (the
  characterization of the latencies returned by the \verb|ping| tool)
  but using your interlocutor's \verb|<router_public_IP_address>|
  instead of \verb|localhost|. Let us consider the generated file as
  \verb|/tmp/<router_public_IP_address>_latencies.txt|. The IPv4
  address of your router can be determined with:
  
  \begin{lstlisting}{language=bash}
curl ipecho.net/plain
  \end{lstlisting}  
  
\item Request to your interlocutor to ping its router from his/her
  private network, for example, using\footnote{Notice that the private
    IP addres of your router could be different.}:
  
  \begin{lstlisting}{language=bash}
ping -s <chunk_length_in_bytes> 192.168.1.1
  \end{lstlisting}
  
  and to send these data to you. Save this info in
  \verb|/tmp/<router_private_IP_address>_latencies.txt|.

\item Supposing that the latencies are symmetric (the direction of the
  packes does not affect the latency) and that the overall network
  latency of the link between you a your interlocutor is the sum of
  the latency from your host to the router of your interlocutor added
  to the latency from your interlocutor's host to that router
  (remember to edit the next line to use the right namefiles):

\texttt{paste /tmp/<router\_public\_IP\_address>\_latencies.txt /tmp/<router\_private\_IP\_address>\_latencies.txt | awk
  \textquotesingle\{print \$1+\$2\}\textquotesingle~> /tmp/add.txt}\\

%\begin{lstlisting}{language=bash}
% | awk '{print \$1+\$2}' > /tmp/add.txt
%  \end{lstlisting}

  and, as in the previous experiment, find a
  characterization for the complete link of the data in \verb|/tmp/add.txt|:
  \href{https://en.wikipedia.org/wiki/Average}{average} (arithmetic
  mean) latency, \href{https://en.wikipedia.org/wiki/Jitter}{jitter},
  \href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson
    correlation coefficient}, and
  \href{https://en.wikipedia.org/wiki/List_of_probability_distributions}{probability
    distribution}.

\end{enumerate}

\subsection{Quantification of the QoE}

Let us measure the QoE using the following classification:
\begin{itemize}
\item \textbf{Perfect}: no loss or delay can be perceived.
\item \textbf{Good}: if you detect some minimal distortion in the
  rendering of the sound.
\item \textbf{Acceptable}: when the effects of the latency are
  noticeable, but you can communicate with your interlocutor.
\item \textbf{Bad}: you are able to recognize only small parts of the
  received audio.
\item \textbf{No way}: when most of the time only silence is heard.
\end{itemize}

\subsubsection{In your host}

Simply quantify your QoE when you run InterCom in your host without
any traffic control.

\subsubsection{In the Internet}

Quantify your QoE when you run InterCom in two different hosts that
are connected to different\footnote{If you are unable to use two
  different LANs, try to run InterCom in two hosts connected
  to the same router.} LANs, without any traffic control.

\subsubsection{In your host, but simulating the Internet}

\begin{enumerate}

\item Check the current configuration:
  
  \begin{lstlisting}{language=bash}
tc qdisc show dev lo
  \end{lstlisting}
  
  The output should be something like: ``\texttt{qdisc noqueue 0: root
    refcnt 2}''
  
\item Using the characterization of the Internet link previously
  obtained, use the command
  \href{https://man7.org/linux/man-pages/man8/tc.8.html}{\texttt{tc}}
  to simulate this link locally using
  \href{https://man7.org/linux/man-pages/man8/tc-netem.8.html}{netem}:

  \begin{lstlisting}{language=bash}
sudo tc qdisc add dev lo root netem delay <average_delay_in_miliseconds>ms <maximum_average_deviation_in_miliseconds>ms <Pearson_correlation_coefficient_expressed_as_a_percentage>% distribution <uniform|normal|pareto|paretonormal>
  \end{lstlisting}
  where:
  \begin{description}
  \item [\texttt{qdisc}:] Use the default
    \href{https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)}{FIFO}
    \href{https://wiki.debian.org/TrafficControl}{Queueing DISCipline}
    for the outgoing traffic.
  \item [\texttt{add}:] Add a new traffic control rule.
  \item [\texttt{dev lo}:] The device affected by the
    rule. \verb|lo| means \verb|loopback|.
  \item [\texttt{root}:] The rule will be applied to all the outbound
    traffic (it's the root rule of the possible tree of rules).
  \item [\texttt{netem}:] Use the
    \href{https://wiki.linuxfoundation.org/networking/netem}{network
      emulator} to emulate a
    \href{https://en.wikipedia.org/wiki/Wide_area_network}{WAN}.
  \end{description}

  Example:

  \begin{enumerate}
  \item Add the following rule:
    
    \begin{lstlisting}{language=bash}
sudo tc qdisc add dev lo root netem delay 100ms 10ms 25% distribution normal
    \end{lstlisting}
    
  \item Check that the rule has been installed with the command:
    
    \begin{lstlisting}{language=bash}
tc qdisc show dev lo
    \end{lstlisting}
    
    that should output (or something very similar): ``\texttt{ qdisc
      netem 8009: root refcnt 2 limit 1000 delay 100ms 10ms 25\%}''
  \end{enumerate}

\item Measure QoE.

\item Delete the \verb|tc| rule with:
  
  \begin{lstlisting}{language=bash}
sudo tc qdisc delete dev lo root netem delay <average_dalay_in_miliseconds>ms <maximum_average_deviation_in_miliseconds>ms <Pearson_correlation_coefficient_expressed_as_a_percentage>% distribution <uniform|normal|pareto|paretonormal>
  \end{lstlisting}

\item (Optional) It is possible to change a working rule with:

  \begin{lstlisting}{language=bash}
sudo tc qdisc change dev lo root netem delay <average_dalay_in_miliseconds>ms <maximum_average_deviation_in_miliseconds>ms <Pearson_correlation_coefficient_expressed_as_a_percentage>% distribution <uniform|normal|pareto|paretonormal>
  \end{lstlisting}
  
\end{enumerate}

\subsection{(Optional) QoE considering the packet loss}

For our application, InterCom, a chunk is lost when it arrives too
late or never arrives. Therefore, the results of a packet loss or delay are almost indistinguishable\footnote{Except by the
  average latency experimented by the user (the higher the network
  latency, the higher the perceived latency).}. For example, a packet
loss ratio of $10\%$ can be configured with \verb|tc| by running:

  \begin{lstlisting}{language=bash}
sudo tc qdisc add dev lo root netem loss 10%
  \end{lstlisting}

\subsection{Control of the buffer size in InterCom}
\label{sec:homework2}
\verb|buffer.py| can be run to intercommunicate two users (or one user
in the case of a simulation), and the buffering time (in miliseconds)
can be controlled with the \verb|--buffering_time| parameter. Using a
real o a simulated environment, find the minimum buffering time that
allows you to hide\footnote{Allowing a good quality of the sound.} the
jitter in your environment. If you are using a simulated one, find
such a minimal buffering time in different jitter configurations
(value, correlation, and distribution). Experiment also with the loss
of chunks to listen to the effect of playing the empty chunks.

\subsection{Modify the \texttt{buffer.py} module}

Finally, remember that the class \texttt{Buffering} in InterCom should
inherit \texttt{NAT\_Traversal} instead of \texttt{Minimal}.

\bibliography{networking,nat}
