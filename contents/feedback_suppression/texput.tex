% Emacs, this is -*-latex-*-

\title{Feedback suppression}

\maketitle

In this milestone, we'll solve the problem generated by the feedback
of the signal emitted by our speakers that is recorded by out
microphone. First, we'll formalize the problem and then look at
different solutions, with varying effectiveness and computational
requirements.

\section{The problem}

One of the first problems we encounter with the use of the
\texttt{buffer.py} module\footnote{And obviously, any other parent
  version of \texttt{buffer.py}.} is that, if we don't use headphones,
the sound that comes out of our PC's (loud)speaker some time later reaches
our mic(rophone), and more some time later, that sound reaches our
interlocutor (the ``far-end'' ... in the system we are the
``near-end'') in the form of an echo (signal) of its own voice, which
is reproduced by his/her speaker, which can be captured again (some
time later) by his/her mic and sent it back to us ... and so on,
generating a rather unpleasant feedback signal.

To formalize the problem, let's define:
\begin{enumerate}
\item $s$ the (analog) signal played by our
  speaker and that reaches our mic,
\item $n$ the signal emited by
  the near-end person (that's me) that reaches my mic\footnote{I.e.,
  the same signal that would be captured by our mic if I were using a
  headset.}, and
\item $m$ the (mixed) signal recorded by my microphone.
\end{enumerate}
In this situation, we have that
\begin{equation}
  m(t) = n(t) + s(t),
  \label{eq:echo_problem}
\end{equation}
where $m(t)$ is the analog audio signal that makes the membrane of our
microphone oscillate.

Our problem here is to minimize the
\href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{energy}
of $s(t)$, i.e., to make
\begin{equation}
  s(t) = 0.
\end{equation}

\section{The trivial (and most efective) solution}

Use a \href{https://en.wikipedia.org/wiki/Audio_headset}{headset}. In
this case,
\begin{equation}
  m(t) \approx n(t)
  \label{eq:headset_solution}
\end{equation}
because $s(t)\approx 0$.

\section{The trivial (but limited) solution}

Decrease the gain of the amplifier of your speaker to do (the energy
of) $s(t)$ as small as possible. Unfortunately, this also decreases the
volume of the far-end signal (the voice of our interlocutor) :-/

\section{The ``simplest'' solution}
Lets ${\mathbf m}$ the digital version of $m(t)$, and ${\mathbf m}[t]$
it's $t$-th sample\footnote{Or frame if we work in stereo}. In this
solution, we send
\begin{equation}
  \tilde{\mathbf n}[t] = {\mathbf m}[t] - a{\mathbf s}[t-d],
  \label{eq:simplest}
\end{equation}
where $a$ is an attenuation (scalar) value, and $d$ represents the
delay\footnote{In a digital signal the sample index indicates the
  position of the sample in the sequence of samples. If we also know
  the sample-time, i.e., the cadency of the sampler, we can also
  compute at which time was taken a sample.} (measured in
sample-times) required to propagate the sound waves from our speaker
to our mic. We define
\begin{equation}
  \hat{\mathbf s}[t] = a{\mathbf s}[t-d]
  \label{eq:minimal_filter}
\end{equation}
as the estimated\footnote{We use the word ``estimated'' because in
this model we are ignoring several factors, such as echoes, unflat
frequency responses, etc.  that modify the version of $\mathbf{s}$
that is received by the microphone.} feedback signal that reaches our
microphone at the same instant of time that the sample ${\mathbf
  n}[t]$ would have been captured in the ausence of the feedback.

Notice that it have been used the notation $\tilde{\cdot}$ to
highlight that $\tilde{\mathbf n}$ is an approximation of ${\mathbf
  n}$ (our sampled true-voice signal), and the notation $\hat{\cdot}$
to emphasize that $\hat{\mathbf s}$ is a registered\footnote{That
matches in time. That are synchronized.} prediction for $s$ reaching
our microphone. Notice also that, if ${\mathbf s}[0]$ is the first
sample of a chunk ($c$-th chunk), the sample ${\mathbf s}[-d]$ could
belong to a previous chunk (the $(c-1)$-th chunk).

Finally, $a$ should be choosen considering that under ausence of voice
in each end, $s(t)\approx 0$. For example, Skype finds $d$ and $a$
using a ``call-signal'' (a sequence of more-or-less
\href{https://en.wikipedia.org/wiki/Musical_tone}{tonal sounds}). $d$
is determined measuring the propagation time of the call-signal
between our speaker and our mic.

\section{Considering the frequency response of the near-end to estimate the feedback signal}
We can improve the performance of the previous feedback cancellation
solution if we take also into consideration that the feedback signal
that finally reaches our microphone is the
\href{https://en.wikipedia.org/wiki/Convolution}{convolution} of
$s(t)$ and a signal $h(t)$ that represents the echo response of our
local audioset (speaker, mic, walls, monitor, keyboard, our body, ...)
to a \href{https://en.wikipedia.org/wiki/Impulse_response}{impulse
  signal} $\delta(t)$.\footnote{This technique is similar to the
  carried out by submarines when they user the sonar to perform
  echo-location, or by bats to fly in the darkness.} In other words,
we can modify Eq.~\eqref{eq:simplest} to compute
\begin{equation}
 \tilde{\mathbf n}[t] = {\mathbf m}[t] - ({\mathbf s}\ast{\mathbf h})[t-d],
  \label{eq:using_convolution}
\end{equation}
where $\ast$ represents the (digital) convolution between (in our case
of) digital signals, and ${\mathbf h}$ is the digitalized (sampled +
quantized) version of $h(t)$, the response (echo signal) of the near-end
(our) audioset to the impact of $\delta(t)$.

The convolution of digital signals in the time domain can be expensive
(with
\href{https://en.wikipedia.org/wiki/Computational_complexity_theory}{computational
  complexity} $O^2$, where $O$ is the number of elements to process)
if the number of samples or/and filter coefficientsis is
high. Fortunately, thanks to the
\href{https://en.wikipedia.org/wiki/Convolution}{convolution theorem}
\cite{kovacevic2013fourier,Oppenheim2}, the convolution can be
replaced by the \href{https://en.wikipedia.org/wiki/Dot_product}{dot
  product} (with complexity $O$), when we consider the signals in the
frequency domain. Thanks to this, we can rewrite the
Eq.~\eqref{eq:using_convolution} as
\begin{equation}
  \tilde{\mathbf n}[t] = {\mathbf m}[t] - ({\mathcal F}^{-1}\{{\mathbf S}{\mathbf H}\})[t-d],
  \label{eq:faster}
\end{equation}
where ${\mathbf S}$ is the (digital) Fourier transform\footnote{The
Fourier transform is an special case of the Laplace transform where
$\sigma=0$ in the complex (Laplace) domain represented by
$s=\sigma+j\omega$ frequencies. This simplification can be used for
the characterization of our near-end audioset because it can be
considered as a FIR (Finite Impulse Response) system (in ausence of an
audio signal, the echo signal always decays with the time). Finally,
notice that ${\mathbf S}[\omega]$ (the $\omega$-th frequency component
of ${\mathbf S}$) is a complex number (the Fourier coefficients are
complex numbers).} of ${\mathbf s}$, ${\mathbf H}$ is the Fourier
transform\footnote{For computing the Fourier transform of digital
signals use a library such as
\href{https://numpy.org/doc/2.1/reference/routines.fft.html}{\text{numpy.fft}}.}
of ${\mathbf h}$, and ${\mathcal F}^{-1}$ represents the inverse
(digital) Fourier transform. Notice that all these transforms are
applied to digital signals, and there exist fast algorithms (with
complexity $O\log_2O$).

Finally, see that this feedback supression technique could ease the
calculation of $d$ because if
\href{https://en.wikipedia.org/wiki/Correlation}{correlation} is used
to estimate it, the maximun should be clearly identified.

\section{Adaptive filtering}
Unfortunately, none of the previous solutions is ``dynamic'', in the
sense that if we modify the audioset, some parameters must be modified
in real-time:
\begin{enumerate}
\item In Equations \eqref{eq:simplest} and
  \eqref{eq:using_convolution}, the parameter $d$ which expresses (in
  sample-times) the time that $s$ need to travel from the speaker to
  the microphone, depends, for example, on the orientation of the
  screen of the laptop.
\item Eq.~\eqref{eq:using_convolution} and (obviously)
  Eq~\eqref{eq:faster} are also affected by $h$, which depends on
  current audioset configuration.
\end{enumerate}

Filters can be implemented in the frequency domain (such as happens in
Eq.~\eqref{eq:faster}) or in the signal (time in our case)
domain. Signal domain (digital) convolutions are affordable if the
length\footnote{The number of coefficients.} of the (digital) filters
is small.\footnote{Take in mind that convolution is a $O^2$ operation,
  and therefore, we can only handle in real-time with our computers
  small filter.}

Eq.~\eqref{eq:minimal_filter} represents the minimal expression for a
\href{https://en.wikipedia.org/wiki/Finite_impulse_response}{FIR
  filter} (whose spectral shape is completely flat because it only has
a single coefficient and therefore, it attenuates equally all the
frequencies). In general, the expression of a FIR filter in the
time-domain (applied to a signal that previously has been delayed $d$
samples) is
\begin{equation}
  \hat{\mathbf s}[t] = \sum_{k=0}^{N-1}{\mathbf h}_k{\mathbf s}[t-k-d],
  \label{eq:FIR_filter}
\end{equation}
where $N$ is the number of coefficients of the filter ${\mathbf h}$,
and ${\mathbf h}_k$ is its $k$-th coefficient. When the (coefficients
of the) filter are updated for each input sample,
Eq.~\eqref{eq:FIR_filter} can be rewritten as
\begin{equation}
  \hat{\mathbf s}[t] = \sum_{k=0}^{N-1}{\mathbf h}_k^{(t)}{\mathbf s}[t-k-d].
  \label{eq:adaptive_FIR_filter}
\end{equation}

In general, the filter should not be updated so often (e.g., every
second could be fast enough). In this case
Eq.~\eqref{eq:adaptive_FIR_filter} should be
\begin{equation}
  \hat{\mathbf s}[t] = \sum_{k=0}^{N-1}{\mathbf h}_k^{(i)}{\mathbf s}[t-k-d].
  \label{eq:adaptive_FIR_filter_second}
\end{equation}
where $i$ represents the update iteration.

\section{Filter update using \href{https://en.wikipedia.org/wiki/Least_mean_squares_filter}{LMS (Least Mean Squares)}}

The LMS algorithm~\cite{haykin1995adaptive,boyd2004convex} was
invented by professor Bernard Widrow and his first Ph.D. student, Ted
Hoff, to train the
\href{https://en.wikipedia.org/wiki/ADALINE}{ADALINE} artificial
neural ``network''.\footnote{See
  \url{https://www.youtube.com/watch?v=hc2Zj55j1zU}} Using LMS,
ADALINE is able to distinguish between patterns, even using \emph{only} a
part of a single neuron.\footnote{If we do not consider the
  \href{https://en.wikipedia.org/wiki/Activation_function}{activation
    function}, an artificial neuron and a FIR filter perform the same
  computation.}

LMS (watch the video) defines that
\begin{equation}
  {\mathbf h}^{(i+1)}_k = {\mathbf h}^{(i)}_k + 2\mu\xi^{(i)}{\mathbf m}[k] \label{eq:update}
\end{equation}
where the error is
\begin{equation}
  \xi^{(i)} = \sum_k({\mathbf n}^{(i)}[k] - {\mathbf m}^{(i)}[k])^2, \label{eq:LMS_error}
\end{equation}
where $i$ represents the chunk number, and $\mu$ is the learning
rate\footnote{High $\mu$ values spped-up the adaption process, but can
generate worse $\mathbf{h}$ coefficients.}.  These equations can be
found\footnote{Again, please, watch
\url{https://www.youtube.com/watch?v=hc2Zj55j1zU}!} using the
(steepest)
\href{https://en.wikipedia.org/wiki/Gradient_descent}{gradient descend
  algorithm}. Unfortunately, Eq.~\eqref{eq:LMS_error} is
un-implementable because $\mathbf{n}$ is unknown. However, we can
approximate it by assuming that the error signal is basically the
sound generated by the speaker, i.e.,
\begin{equation}
  \xi^{(i)} = \sum_k({\mathbf s}^{(i)}[k])^2. \label{eq:the_error_is_the_speaker}
\end{equation}

\section{Filter update using a train of impulses}
If we decide to work with impulses, ${\mathbf h}^{(i)}$ is determined
by the samples recorded by our soundcard after $d$ sample-times of the
generation of each impulse when the impulses are the only source of
sound. If there were other sources of sound (such as for example, our
voice) the updates should be discarded or averaged using some
\href{https://en.wikipedia.org/wiki/Moving_average}{moving-average}
technique.

\section{The substract solution in the frequency domain}
Ultimately, feedback in InterCom occurs because the audio signal we
are generating is reproduced through our speakers (with a certain time
lag). In this feedback situation, the spectrum of a newly received
chunk is similar to the spectrum of a newly sent chunk. Therefore, if
the chunk we want to send is pre-filtered (before being sent) using
the opposite filter to that described by the spectrum of the received
chunk, we should cancel the feedback.

Let ${\mathbf M} = {\mathcal F}({\mathbf m})$ the Fourier transform of
the $i$-th chunk captured by the soundcard, and let
${\mathbf S} = {\mathcal F}({\mathbf s})$ the Fourier transform of the
$i$-th played chunk. Considering the reasoning followed in the
previous paragraph, and considering also the Eq.~\eqref{eq:simplest}, it can be concluded that
\begin{equation}
  \tilde{\mathbf N} = {\mathbf M} - a{\mathbf S},
  \label{eq:simplest_fourier}
\end{equation}
where $\tilde{\mathbf N} = {\mathcal F}({\mathbf n})$, the Fourier
transform of the chunk that should be sent in the $i$-th iteration
(chunk).

\begin{comment}
Notice that if you prefer to estimate ${\mathbf h}$ using a ``train''
(a sequence) of impulses, the temporal distance between them would 

  For estimating the feedback signal at the
  sample-time $t$, ${\mathbf f}[t]$, the length of a
  \href{https://en.wikipedia.org/wiki/Finite_impulse_response}{FIR
    filter} should be at least $d$, because at least we need $d$
  samples to detect the feedback signal. Therefore, we have that
\begin{equation}
  \hat{\mathbf s}[t] = \sum_{k=0}^{d-1}{\mathbf h}_k^{(t)}{\mathbf s}[t-k],
  \label{eq:LMS_feedback}
\end{equation}
where ${\mathbf h}$ is the near-end impulse response in the time
domain.

\section{Estimation of the feedback signal using LMS (Least Mean Squares)}
We just have seen how it is possible to find better estimations of the
feedback signal $\hat{\mathbf f}$ using convolutions. By definition,
convolutions are performed by filters. 

As we have seen, 

Also, as we have seen in the previous section, it is possible to adapt
the filter to the acoustic conditions, measuring the echo generated by
the impulse signal. In the time domain, one of the most used
techniques for computing the coefficients of a FIR filter is the
\href{https://en.wikipedia.org/wiki/Least_mean_squares_filter}{LMS
  (Least Mean Squares)
  algorithm}~\cite{haykin1995adaptive,boyd2004convex}, among other
reasons because the filter (coefficients) can be adapted to variations
in the signal to filter (the filter can learn).

LMS can be used to compute the coefficients of a filter to provide a
desired output for a given input. In our
context, the input signal is ${\mathbf m}$ (the digital signal
recorded by our soundcard) and the desired output signal is
${\mathbf n}$ (the digital version of our voice). LMS, iteratively
computes
 Notice that we process the signals sample-by-sample (at
iteration $i$ we compute the $i$-th sample of $\tilde{\mathbf n}$ (the
signal without the feedback, supposely containing \emph{only} our
voice), and this is the signal that we will send to the far-end in the
next chunk).

\section{Filtering in the frequency domain}

Assume that $s$ (the signal received by our microphone from our
speaker) has essentially the same frequency spectrum as the audio
signal being played (i.e., $h(t)=\delta(t)$, and therefore, the
environment does not modify the spectrum of the reproduced signal). We
denote the signal reproduced by our speaker as $f(t)$ ($f$ for
far-end). In this case, we can propose a feedback reduction system by
applying a filter $\mathbf{F}^{-1}$ to the captured signal
$\mathbf{s}$, which is equal to the ``inverse'' of the spectrum of
$\mathbf{f}$ (the signal we are reproducing). Specifically, this would
involve:

\begin{enumerate}
\item \textbf{Calculating} $\mathbf{F}$, the Fourier transform of the
  reproduced audio signal $\mathbf{f}$, chunk by chunk.
\item \textbf{Calculating} $\mathbf{F}^{-1}$, the ``inverse'' of
  $\mathbf{F}$. If $w_{\text{max}}$ is the frequency component of
  $\mathbf{F}$ with the highest energy, $g_{\text{max}}$, i.e.,
  $\mathbf{F}[w_{\text{max}}]=g_{\text{max}}$, and
  $\mathbf{F}[w_{\text{min}}]=g_{\text{min}}$, normalize its spectrum
  with:
  \begin{equation}
    \mathbf{F}_{\text{normalized}} = \frac{\mathbf{F} - g_{\text{min}}}{g_{\text{max}} - g_{\text{min}}},
  \end{equation}
  and finally calculate:
  \begin{equation}
    \mathbf{F}^{-1} = 1 - \mathbf{F}_{\text{normalized}}.
  \end{equation}
\item \textbf{Filtering the signal captured by the microphone} using
  $\mathbf{F}^{-1}$, taking into account the attenuation factor $a$,
  which relates how much of the reproduced signal is captured by the
  microphone (how much of $\mathbf{f}$ is in $\mathbf{s}$), i.e.,
  calculate:
  \begin{equation}
    \hat{\mathbf{N}} = a\mathbf{F}^{-1}\mathbf{S},
  \end{equation}
  where $\mathcal{F}(\mathbf{s})=\mathbf{N}$.
\item \textbf{Estimating the attenuation factor} $a$ for the next
  chunk. Initially, we assume that $a=1$ (and therefore, that there is
  no attenuation). If it is an overestimation, then some frequency
  components in the sent audio could disappear, and if we
  underestimate, then some frequency components could be
  amplified. Therefore, the attenuation factor should be high enough
  so that the energy of $\mathbf{s}$ does not increase progressively,
  but low enough so that some frequency components do not become 0.
\end{enumerate}
The signal to send (without the feedback) is:
\begin{equation}
  \mathbf{n} = \mathcal{F}^{-1}(\mathbf{N})
\end{equation}

\end{comment}

\section{Deliverables}

\begin{enumerate}
\item
\textbf{Write a Python module} called \texttt{feedback\_supression.py} that inherits
from \texttt{buffer.py} and that implements at least\footnote{More
  working implementations, higher grade.} one of the previously
described solutions.
\textbf{Evaluate it} comparing with other feedback supression solutions such as the provided by Google Meet.
\end{enumerate}

\begin{comment}
More concretely:

\begin{enumerate}

\item If you implement the ``$s(t)$ delay and subtract solution'',
  you must estimate $a$, $d$ and perform the signals subtraction to
  remove the feedback signal ($s(t)$). 

\item In a ``$s(t)$ delay and subtract solution'', if you also
  consider the frequency response of the near-end audioset to estimate
  a better feedback signal, first you will need to find ${\mathbf H}$ (the
  discrete frequency response of your audioset). For this, the
  near-end speaker should generate an impulse signal
  ${\mathbf \delta}$, and in absence of any other sound signal, record
  the echo and compute its Fourier transform (it is a good idea to
  repeat this process several times to obtain a better estimation of
  ${\mathbf H}$). Finally, .

  Notice that the frequency characterization of the far-end audioset
  can be also used in a ``$n(t)$ delay and subtract
  solution''. Remember that filtering operations must be implemented
  with convolutions in the temporal domain, but are dot products in
  the frequency domain.
  
\item Use LMS to find a estimation of the echo signal and perform the
  echo cancellation. In this case, consider the use of a thread to
  compute the coefficients of the LMS filter
  (Equation~\eqref{eq:update} can run with a cadence much higher than
  a sample-time) and compute the estimated feedback signal
  (Equation~\eqref{eq:LMS_feedback}) for every chunk. Notice that for
  doing that, we will require the first $k$ samples of the next chunk.

\item 
\end{enumerate}

Optionally, but interesting for your mark, use any other technique
(for example, an artificial neural network\footnote{ADALINE is the
  simplest
  \href{https://en.wikipedia.org/wiki/Neural_network_(machine_learning)}{AAN}
  ever developed!}) for estimating the echo, and use it for removing
the echo (obviously, in real-time). Take also into consideration that
the parameters that determine the estimation of the echo signal should
be continously\footnote{A 1-seconds cadence should be enought.}
monitored becase the physical composition of the audiosets can
be dynamic (for example, the inclination of the screen of our laptop
can be modified at any moment).

Finally, notice that
operation can help to fine-tune $d$.

% Mark: \textbf{10 points}.

\item To the report describing how your module works, \textbf{add a
    section evaluating it} (with a ratio out of 10), considering that
  each of the following rubrics has the same weight in the evaluation
  score:
  \begin{enumerate}
  \item Compared, for example, with the feedback supression provided
    by Google Meet, how of effective is your proposal? You must
    describe how this comparison has been carried out.
  \item  Is your implementation efficient in terms of CPU usage? Explain.
  \item Is the code of the module properly explained and
    documented (in the own module)?
  \end{enumerate}
\end{enumerate}
\end{comment}

\section{Resources}

\bibliography{signal_processing,optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{comment}
  At the same time you can also implement a ``$n(t)$ delay and
  subtract solution'', where you have to estimate $a$ for your voice
  $n(t)$ delayed the buffering time + average RTT + $d$ at the
  far-end. The average RTT can be estimated using\footnote{Remember
    that \texttt{ping} only works between public devices.} a local
  clock, storing in a list when the last $N$ chunks were sent, and
  sending with each chunk a new field in the packet header with the
  last chunk number that was received. Thus, when we receive the chunk
  $n$-th\footnote{Remember that the chunks have a chunk number in the
    packet header.}, we can copy this number in the next sent chunk
  and the receiver will be able to find the RTT subtracting to the
  time reception the corresponding time in the list.
  \end{comment}


\begin{comment}
http://www.seas.ucla.edu/dsplab/index.html
https://es.mathworks.com/help/signal/ug/echo-cancelation.html
https://dsp.stackexchange.com/questions/26617/echo-cancelling-using-autocorrelation-function
https://pypi.org/project/adaptfilt/
http://www.diva-portal.org/smash/get/diva2:280596/fulltext01
https://github.com/ThomasHaubner/e2e_dnn_ad_control_for_lin_aec
https://scicoding.com/4-ways-of-calculating-autocorrelation-in-python/



where ${\mathbf m}[i]$ is the $i$-th sample of the signal
${\mathbf m} = \{{\mathbf m}[i]\}$, $a$ is a real number that expresses
an attenuation\footnote{Usually, $a<1$ because in general the amount
  of signal that our micro captures from our speaker is smaller (in
  \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{energy}~\cite{vetterli2014foundations})
  than the signal ${\mathbf s}$.}, and $d$ is an integer number that
indicates the delay (in sample-times\footnote{A sample-time is the
  interval of time that separates two consecutive samples (or frames)
  and depends on the sampling frequency.}) that exists between when
the received signal ${\mathbf s}$ is reproduced by our speaker until
such signal is captured by our ADC (Analog Digital Converter).

\section{The trivial partial solution}
One way to minimize this problem\footnote{Apart from using headphones,
  which is by far the better solution.} is to reduce as much as
possible (as long as it is audible, of course) the gain of the
amplifier that feeds our speaker(s). In terms of the
Eq.~\eqref{eq:echo_problem}, this means to make $a$ close to
zero. Unfortunately, this is not always possible, among other reasons,
because if the volume is too low, we will not hear our interlocutor
(the ``far-end'').


\section{Delay and subtract solution}

Another simple\footnote{The AEC (Audio Echo Cancellation) problem has
  been extensively estudied and there are several solutions. This one
  is probably the simplest one.} solution is to determine $a$ and $d$,
and compute
\begin{equation}
  {\mathbf n}[i] = {\mathbf m}[i] - a{\mathbf s}[i-d].
  \label{eq:echo_cancellation}
\end{equation}

$d$ can be found by measuring the time that a played signal spends to
be recorded by our mic, and $a$ computing the ratio between the
energies of ${\mathbf s}$ (the played signal) and ${\mathbf m}$ (the
recorded signal). For example, Skype finds $d$ and $a$ at the
beginning of the session using a ``call signal'' (a sequence of
more-or-less tonal sounds).

\subsection{Considering the frequency respose of the near-end audioset}
In general, Eq.~\eqref{eq:echo_problem} is oversimplified because our
local audioset (speaker, mic, walls, our body, etc.) does not have a
flat response in the frequency domain. Therefore, a more realistic
model of the echo effect is
\begin{equation}
   {\mathbf m}[i] = {\mathbf n}[i] + \{f({\mathbf s})\}[i-d],
  \label{eq:more_realistic_echo_problem}
\end{equation}
where $f({\mathcal s})$ is a filtered version of the signal
${\mathbf s}$, in which some frequencies are partially attenuated
(notice that now there is a possiblely different $a$-value for each
frequency of ${\mathbf s}$).

Now the problem (apart from finding $d$, obviously) is how to
determine $f(\cdot)$. Again, a simple way of doing this is to use a
call signal to measure the frequency response of the near-end
audioset. For this, a good call signal can be a sequence of
impulses\footnote{A impulse generates a flat
  \href{https://en.wikipedia.org/wiki/Spectral_density}{spectrum}~\cite{kovacevic2013fourier,Oppenheim2}
  for a short period of time.} of a sequence of uniform random noise
signals\footnote{Uniform random noise (or
  \href{https://en.wikipedia.org/wiki/White_noise}{white noise}) has a
  flat spectrum.}. Notice that the frequency response (the filter
coefficients of $f(\cdot)$) can be found comparing the flat spectrum
signal ${\mathbf s}$ generated by the speaker with the supposedly
non-flat spectrum of the signal ${\mathbf m}$ captured by the
microphone. To apply $f(\cdot)$ we can multiply
${\mathcal F}({\mathbf s})$ by the filter coefficients of $f(\cdot)$
in the Fourier domain, being ${\mathcal F}(\cdot)$ the Fourier
Transform~\cite{kovacevic2013fourier,Oppenheim2}.


Lets $H(s)$ the representation of $h(t)$ in the frequency (Laplace)
domain, where $s=\sigma+j\omega$ is a complex number that denotes a
frequency component.

If we consider that $h(t)$ in the frequency domain is denoted by
$H(s)$, (when $s=\sigma+j\omega$ we say that $H(s)$ is the Laplace
Transform of $h(t)$), that $S(s)$ is the representation in the
frequency domain of the signal $s(t)$, and that in our specific case,
where our near-end audioset is a FIR (Finite Impulse Response)
system\footnote{In ausence of an audio signal, the echo always decays
  with the time.}, we can also characterize it using the Fourier
Transform (where $sigma=0$), i.e., we can say that $H(j\omega)$ (the
Fourier Transform of $h(t)$) describes our local audioset from a
frequency perspective. Considering now the convolution theorem in the
Fourier domain, we can rewrite the Eq.~\eqref{eq:simplest} as
\begin{equation}
  \tilde{\mathbf n}[i] = {\mathbf m}[i] - {\mathbf h}({\mathbf S}{\mathbf H})[i-d],
  \label{eq:simplest2}
\end{equation}
where ${\mathbf S}$ is the Fast Discrete Fourier Transform of
${\mathbf s}$ and ${\mathbf H}$ is the Fast Discrete Fourier Transform
of ${\mathbf h}$, and ${\mathcal F}^{-1}$ represents the Fast Inverse
Discrete Fourier Transform.

Therefore, as it can be seen in Eq.~\eqref{eq:simplest2}, to find the
echo signal we need to mulplity  ${\mathbf S}$ (the played signal in the Fourier
domain) by ${\mathbf H}$ (the frequency response of the near-end
audioset).

The LMS (Least Mean Squares) algorithm is an algorithm designed to
minimize the mean square error of a target function.


LMS is adaptive,
which means that it suitable when there are near-end acoustic
variations. In the previous section we have computed the near-end
audioset impulse response using the frequency domain. In this case, we
will use the time domain, i.e.,
\begin{equation}
  \hat{\mathbf e}[t] = \sum_{k=0}^{N-1}{\mathbf h}_k^{(t)}{\mathbf s}[t-k]
\end{equation}
where ${\mathbf h}$ is the near-end impulse response and $N$ is the
length of the LMS filter.

LMS minimizes the energy of the residue signal
\begin{equation}
  r[t] = {\mathbf m}[t] - {\mathbf n}[t-d]
\end{equation}
(the difference between the signal captured by the mic and the signal
generated by the near-end user) that ideally, after a delay of $d$
samples (an input parameter of this technique) should be the echo
signal. More concretely, LMS minimizes $E\{|r|^2_2\}$ (the expectation
of the square of $r$) using an interative algorithm where, in the
iteration $i$ computes
\begin{equation}
  h^{(i+1)} = h^{(i)} + \mu r^{(i)}{\mathbf m}[i],
\end{equation}
where
\begin{equation}
  r^{(i)} = {\mathbf s}[i] - \hat{\mathbf e}[i],
\end{equation}
and $\mu$ is the ``learning rate'' of the LMS algorithm.


\end{comment}
