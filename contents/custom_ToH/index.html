<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Custom Thresholds of Hearing</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Custom Thresholds of Hearing</h2>
 <div class='author'><a href='https://vicente-gonzalez-ruiz.github.io/'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>&amp; </span><a href='https://www.marcoslupion.com/'><span class='ecrm-1200'>Marcos Lupión Lorente</span></a></div><br />
<div class='date'><span class='ecrm-1200'>February 1, 2024</span></div>
   </div>
   <h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-10001'></a>Description</h3>
<!-- l. 8 --><p class='noindent'>In general, the ToH (Threshold of Hearing) is different for each person. Added to this, not all
the auditory information that is received from our iterlocutor is transduced with the same
fidelity<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-1001f1'></a>
Knowing this, we can take advantage of this fact by avoiding having to transmit
information that we will not be able to perceive. In other words, we can compute a
set of quantization step sizes that our interlocutor can use to quantize the audio that
he/she is going to send us.
</p><!-- l. 19 --><p class='indent'>   To find such QSSs, we can simulate <a href='https://en.wikipedia.org/wiki/Quantization_(signal_processing)'>quantization noise</a> <span class='cite'>[<a href='#Xsayood2017introduction'>1</a>]</span> in each subband <span class='cite'>[<a href='#Xvetterli1995wavelets'>2</a>]</span>
and use the amplitude of the noise when it starts to be noticeable. Supposing that
the quantization noise follows an <a href='https://en.wikipedia.org/wiki/Continuous_uniform_distribution'>uniform distribution</a>, the next algorithm can be
implemented:
</p><!-- l. 28 --><p class='indent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-1004x1'>
     <!-- l. 29 --><p class='noindent'>Let \(\{{\mathbf l}^{N_{\text {levels}}}, {\mathbf h}^{N_{\text {levels}}}, {\mathbf h}^{N_{\text {levels}}-1},\cdots , {\mathbf h}^1\}\) the wavelet representation of a chunk. Starting with \({\mathbf l}^{N_{\text {levels}}}\) (at the first
     iteration the rest of subbands are zero), split the wavelet subband into
     a (configurable) number of Fourier subbands. While the noise stays
     imperceptible:
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-1006x1'>Increase the amplitude of the noise in the Fourier subband.
         </li>
<li class='enumerate' id='x1-1008x2'>Compute the inverse FFT/wavelet-transform.
                                                                  

                                                                  
         </li>
<li class='enumerate' id='x1-1010x3'>Reproduce the generated chunk, alternating every second with the
         play of an empty chunk.</li></ol>
     </li>
<li class='enumerate' id='x1-1012x2'>Continue with the next subband, but keeping the highest unperceptible uniform
     noise in the previously processed ones. In this way, for the subband \({\mathbf h}^1\)
     (the last one to be processed), the rest of subbands should be already
     noisy.</li></ol>
<!-- l. 51 --><p class='indent'>   Again, notice that the QSSs are determined for the sound that you are going to
play (not for the audio that you are generating). Therefore, you should use your
interlocutor’s QSSs and vice versa. It is also necessary the transmission of this
information.
</p><!-- l. 56 --><p class='indent'>   Finally, use the new QSSs instead of the default ones (those generated using the
standard ToH).
</p><!-- l. 59 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-20002'></a>Deliverables</h3>
<!-- l. 61 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.1   </span> <a id='x1-30002.1'></a>Implement <span class='ectt-1000'>find_custom_ToH.py</span></h4>
<!-- l. 63 --><p class='noindent'>Provide an implementation of the previous algorithm for determining your ToH, and
sending it to your interlocutor.
</p><!-- l. 66 --><p class='noindent'>
</p>
   <h4 class='subsectionHead'><span class='titlemark'>2.2   </span> <a id='x1-40002.2'></a>Implement <span class='ectt-1000'>use_custom_ToH.py</span></h4>
<!-- l. 68 --><p class='noindent'>Provide an implementation that receives the “custom” QSSs, and use to encode the
audio that you are sending.
</p><!-- l. 71 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-50003'></a>Resources</h3>
                                                                  

                                                                  
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xsayood2017introduction'></a>K. Sayood.    <a href='http://rahilshaikh.weebly.com/uploads/1/1/6/3/11635894/data_compression.pdf'><span class='ecti-1000'>Introduction  to  Data  Compression</span></a>  <a href='https://people.cs.nctu.edu.tw/~cmliu/Courses/Compression/'><span class='ecti-1000'>(Slides)</span></a>.    Morgan
   Kaufmann, 2017.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xvetterli1995wavelets'></a>M. Vetterli  and  J. Kovačević.     <a href='http://waveletsandsubbandcoding.org/Repository/VetterliKovacevic95_Manuscript.pdf'><span class='ecti-1000'>Wavelets  and  Subband  Coding</span></a>.
   Prentice-hall, 1995.
</p>
   </div>
<p><a id='Q1-1-6'></a>
   </p><div class='footnotes'><a id='x1-1002x1'></a>
<!-- l. 13 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>For example, your speakers could not have a flat frequency response, or your room could
</span><span class='ecrm-0800'>attenuate more, some frequencies.</span></p>                                                                             </div>
 
</body> 
</html>