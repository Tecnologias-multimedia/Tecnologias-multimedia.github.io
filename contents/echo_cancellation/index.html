<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Echo Cancellation</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Echo Cancellation</h2>
 <div class='author'><a href='https://vicente-gonzalez-ruiz.github.io/'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>&amp; </span><a href='https://hpca.ual.es/~savins/'><span class='ecrm-1200'>Savins Puertas Martín</span></a> <span class='ecrm-1200'>&amp; </span><a href='https://www.marcoslupion.com/'><span class='ecrm-1200'>Marcos Lupión Lorente</span></a></div><br />
<div class='date'><span class='ecrm-1200'>October 4, 2024</span></div>
   </div>
   <h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-10001'></a>The problem</h3>
<!-- l. 9 --><p class='noindent'>One of the first problems we encounter with the use of the <span class='ectt-1000'>buffer.py</span>
module<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-1001f1'></a>
is that, if we don’t use headphones, the sound that comes out of our PC’s
speaker reaches our mic(rophone) some time later, and more some time
later, that sound reaches our interlocutor (the “far-end” ... we are the
“near-end”) in the form of an echo (signal), which is reproduced by his/her
speaker, which can be captured again (some time later) by his microphone
and sent it back to us ... and so on, generating a rather unpleasant signal.
In other words, if \(\mathbf s\) is the (analog) signal played by our speaker and that
reaches our mic, \(\mathbf n\) is the signal emited by the near-end person that reaches our
mic<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-1003f2'></a>,
and \(\mathbf m\) is the (mixed) signal recorded by our microphone, we have that \begin {equation}  m(t) = n(t) + s(t), \label {eq:echo_problem}  \end {equation}<a id='x1-1005r1'></a> where \(m(t)\) is the
signal that hits the membrane of our mic.
</p><!-- l. 30 --><p class='indent'>   Our problem here is to minimize the energy of \(s(t)\).
</p><!-- l. 32 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-20002'></a>The trivial (and definitive) solution</h3>
<!-- l. 34 --><p class='noindent'>Use a <a href='https://en.wikipedia.org/wiki/Audio_headset'>headset</a>. In this case, \begin {equation}  m(t) \approx n(t) \label {eq:headset_solution}  \end {equation}<a id='x1-2001r2'></a> because \(s(t)\approx 0\).
</p><!-- l. 42 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-30003'></a>The trivial (but limited) solution</h3>
                                                                  

                                                                  
<!-- l. 44 --><p class='noindent'>Decrease the gain of the amplifier of our speaker to do \(s(t)\approx 0\). Unfortunately this also
decreases the volume of the far-end signal (the voice of our interlocutor)
:-/
</p><!-- l. 48 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4   </span> <a id='x1-40004'></a>The simplest substract solution</h3>
<!-- l. 49 --><p class='noindent'>Lets \(\mathbf m\) the digital version of \(m(t)\) and \({\mathbf m}[t]\) it’s \(t\)-th
sample<span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-4001f3'></a>.
In this solution, we send \begin {equation}  \tilde {\mathbf n}[t] = {\mathbf m}[t] - a{\mathbf s}[t-d], \label {eq:simplest}  \end {equation}<a id='x1-4003r3'></a> where \(a\) is an attenuation (scalar) value and \(d\) represents the
delay<span class='footnote-mark'><a href='#fn4x0' id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-4004f4'></a>
(measured in sample-times) required to propagate the sound from our speaker to our
mic. We define \begin {equation}  \hat {\mathbf e}[t] = a{\mathbf s}[i-d]  \end {equation}<a id='x1-4006r4'></a> as the estimated echo signal.
</p><!-- l. 68 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5   </span> <a id='x1-50005'></a>Considering the frequency response of the near-end to estimate the echo
signal</h3>
<!-- l. 69 --><p class='noindent'>We can improve the performance of the echo cancellation process if we take also into
consideration that echo signal that finally reaches our mic is the <a href='https://en.wikipedia.org/wiki/Convolution'>convolution</a>
of \(s(t)\) and a signal \(h(t)\) that represents the echo response of our local audioset
(speaker, mic, walls, monitor, keyboard, our body, etc.) to a impulse signal
\(\delta (t)\).<span class='footnote-mark'><a href='#fn5x0' id='fn5x0-bk'><sup class='textsuperscript'>5</sup></a></span><a id='x1-5001f5'></a> In
other “words”, we can compute \begin {equation}  \tilde {\mathbf n}[t] = {\mathbf m}[t] - ({\mathbf s}*{\mathbf h})[t-d], \label {eq:using_convolution}  \end {equation}<a id='x1-5003r5'></a> where \(*\) represents the convolution between (in our
case of) digital signals, and \(\mathbf h\) is the digitalized (discrete + quantized) version of \(h(t)\), the
response of the near-end audioset to the impact of \(\delta (t)\).
</p><!-- l. 87 --><p class='indent'>   The convolution of digital signals in the time domain can be expensive (with
<a href='https://en.wikipedia.org/wiki/Computational_complexity_theory'>computational complexity</a> \(O^2\)) if the number of samples is high. Fortunately,
thanks to the <a href='https://en.wikipedia.org/wiki/Convolution'>theorem of the convolution</a> of signals in the frequency
domain <span class='cite'>[<a href='#Xkovacevic2013fourier'>3</a>, <a href='#XOppenheim2'>4</a>]</span>, the convolution can be replaced by the <a href='https://en.wikipedia.org/wiki/Dot_product'>dot product</a> (\(O\)), when we
consider the signals in the frequency domain. Thanks to this, we can rewrite
the Eq. \eqref{eq:using_convolution} as \begin {equation}  \tilde {\mathbf n}[t] = {\mathbf m}[t] - {\mathcal F}^{-1}\{{\mathbf S}{\mathbf H}\}[t-d], \label {eq:faster}  \end {equation}<a id='x1-5004r6'></a> where \(\mathbf S\) is the (digital) Fourier
transform<span class='footnote-mark'><a href='#fn6x0' id='fn6x0-bk'><sup class='textsuperscript'>6</sup></a></span><a id='x1-5005f6'></a>
of \(\mathbf s\), \(\mathbf H\) is the Fourier transform of \(\mathbf h\), and \({\mathcal F}^{-1}\) represents the inverse Fourier transform.
Notice that all these transforms are applied to digital signals, and there exist fast
algorithms (\(O\log _2O\)).
</p><!-- l. 113 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>6   </span> <a id='x1-60006'></a>Estimation of the echo signal using LMS (Least Mean Squares)</h3>
                                                                  

                                                                  
<!-- l. 114 --><p class='noindent'>We just have seen how it is possible to find better estimations of the echo
signal \(\mathbf e\) using convolutions. By definition, convolutions are performed by
filters.
</p><!-- l. 118 --><p class='indent'>   Filters can be implemented in the frequency domain (see the previous section) or in the
signal (time in our case) domain. Signal domain (digital) convolutions are efficient when
the length<span class='footnote-mark'><a href='#fn7x0' id='fn7x0-bk'><sup class='textsuperscript'>7</sup></a></span><a id='x1-6001f7'></a>
of the (digital) filters is small. For determining \(\mathbf e\), the length of a <a href='https://en.wikipedia.org/wiki/Finite_impulse_response'>FIR filter</a> should
be at least \(d\), because at least we need \(d\) samples to detect the echo signal.
Therefore, we have that \begin {equation}  \hat {\mathbf e}[t] = \sum _{k=0}^{d-1}{\mathbf h}_k^{(t)}{\mathbf s}[t-k],  \end {equation}<a id='x1-6003r7'></a> where \(\mathbf h\) is the near-end impulse response in the time
domain.
</p><!-- l. 132 --><p class='indent'>   Also, as we have seen in the previous section, it is possible to adapt the filter to
the acoustic conditions, measuring the echo generated by the impulse signal. In the
time domain, one of the most used techniques for computing the coefficients of a FIR
filter is the <a href='https://en.wikipedia.org/wiki/Least_mean_squares_filter'>LMS (Least Mean Squares) algorithm</a> <span class='cite'>[<a href='#Xhaykin1995adaptive'>2</a>, <a href='#Xboyd2004convex'>1</a>]</span>, among other reasons
because the filter (coefficients) can be adapted to variations in the signal to filter (the
filter can learn).
</p><!-- l. 142 --><p class='indent'>   LMS was invented by professor Bernard Widrow and his first
Ph.D. student, Ted Hoff, to train the <a href='https://en.wikipedia.org/wiki/ADALINE'>ADALINE</a> artificial neural
network.<span class='footnote-mark'><a href='#fn8x0' id='fn8x0-bk'><sup class='textsuperscript'>8</sup></a></span><a id='x1-6004f8'></a>
Using LMS, ADALINE is able to learn pattern, even using a single
neuron<span class='footnote-mark'><a href='#fn9x0' id='fn9x0-bk'><sup class='textsuperscript'>9</sup></a></span><a id='x1-6006f9'></a>.
</p><!-- l. 150 --><p class='indent'>   LMS can be used to compute the coefficients of a filter to provide a desired
output (a signal) for a given input (a signal). In our context, the input signal is \(\mathbf m\) (the
digital signal recorded by our soundcard) and the desired output signal is \(\mathbf n\) (the
digital version of our voice). LMS, iteratively computes </p><div class='eqnarray'>\begin {eqnarray}  {\mathbf h}^{(i+1)} &amp; = &amp; {\mathbf h}^{(i)} + 2\mu \epsilon ^{(i)}{\mathbf m}[i-d:i] \\ \epsilon ^{(i) }&amp; = &amp; {\mathbf n}[i-d:i] - {\mathbf m}[i-d:i]^\mathrm {T}{\mathbf h}^{(i)},  \end {eqnarray}</div>
<!-- l. 160 --><p class='indent'>   where \(i\) represents the iteration number, \(\mu \) is the learning rate, and \(\cdot ^\mathrm {T}\) is
the transpose of a matrix. Notice that we are using <a href='https://numpy.org/doc/stable/user/basics.indexing.html'>the NumPy slicing
notation</a>. These equations can be found using the (steepest) <a href='https://en.wikipedia.org/wiki/Gradient_descent'>gradient descend
algorithm</a>.<span class='footnote-mark'><a href='#fn10x0' id='fn10x0-bk'><sup class='textsuperscript'>10</sup></a></span><a id='x1-6008f10'></a>
</p><!-- l. 170 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>7   </span> <a id='x1-70007'></a>Deliverables</h3>
<!-- l. 172 --><p class='noindent'>A Python module called <span class='ectt-1000'>echo_cancellation.py </span>that inherits from <span class='ectt-1000'>buffer.py </span>and
that implements at least one of the previously described solutions. More
concretely:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7002x1'>If you implement the “delay and substract solution”, you must estimate
     \(a\), \(d\) and perform the signals substraction to remove the echo signal. For
     example, Skype finds \(d\) and \(a\) using a “call signal” (a sequence of more-or-less
     tonal sounds). \(d\) is determined measuring the propagation time of the call
                                                                  

                                                                  
     signal between our speaker and our mic, and \(a\) measuring the ratio between
     the  energy  of  the  call  signal  played  and  the  energy  of  the  call  signal
     recorded.
     </li>
<li class='enumerate' id='x1-7004x2'>If  you  also  consider  the  (discrete)  frequency  response  of  the  near-end
     audioset to estimate a better echo signal, first you will need to find \(\mathbf H\) (the
     discrete frequency response of the audioset). For this, the local-end speaker
     should generate an impulse signal \(\mathbf \delta \), and in absence of any other sound
     signal, record the echo and compute its Fourier transform. Likely, it is a
     good idea to repeat this process several times to obtain a better estimation
     of \(\mathbf H\). Finally, notice that \({\mathbf H}[\omega ]\) (the \(\omega \)-th frequency component of \(\mathbf H\)) is a complex
     number.
     </li>
<li class='enumerate' id='x1-7006x3'>In both cases (simple delay and substract solution and considering also
     the frequency response of the near-end audioset), the parameters that
     determine the estimation of the echo signal should be continously<span class='footnote-mark'><a href='#fn11x0' id='fn11x0-bk'><sup class='textsuperscript'>11</sup></a></span><a id='x1-7007f11'></a>
     monitored becase the physical composition of the near-end audioset can
     be dynamic (for example, the inclination of the screen of our laptop can
     be modified).
     </li>
<li class='enumerate' id='x1-7010x4'>Use LMS to find a estimation of the echo signal.
     </li>
<li class='enumerate' id='x1-7012x5'>Finally, use any other technique (for example, an artificial neural network)
     for estimating the echo.</li></ol>
<!-- l. 206 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>8   </span> <a id='x1-80008'></a>Resources</h3>
   <div class='thebibliography'>
                                                                  

                                                                  
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xboyd2004convex'></a>Stephen  Boyd  and  Lieven  Vandenberghe.     <a href='https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf'><span class='ecti-1000'>Convex  Optimization</span></a>.
   Cambridge University Press, 2004.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xhaykin1995adaptive'></a>S. Haykin. <a href='https://users.ics.forth.gr/~tsakalid/UVEG09/Book/Haykin-AFT(3rd.Ed.)_Introduction.pdf'><span class='ecti-1000'>Adaptive Filter Theory (3rd edition)</span></a>. Prentice Hall, 1995.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xkovacevic2013fourier'></a>J. Kovačević,  V.K.  Goyal,  and  M. Vetterli.   <a href='https://foundationsofsignalprocessing.org/FWSP_a3.2_2013.pdf'><span class='ecti-1000'>Fourier and Wavelet
   </span><span class='ecti-1000'>Signal Processing</span></a>. <a class='url' href='http://www.fourierandwavelets.org/'><span class='ectt-1000'>http://www.fourierandwavelets.org/</span></a>, 2013.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [4]<span class='bibsp'>   </span></span><a id='XOppenheim2'></a>Alan V. Oppenheim, Alan S. Willsky, and S. Hamid Nawab.  <a href='http://materias.df.uba.ar/l5a2021c1/files/2021/05/Alan-V.-Oppenheim-Alan-S.-Willsky-with-S.-Hamid-Signals-and-Systems-Prentice-Hall-1996.pdf'><span class='ecti-1000'>Signals
   </span><span class='ecti-1000'>and Systems (2nd edition)</span></a>. Prentice Hall, 1997.
</p>
   </div>
   <div class='footnotes'><a id='x1-1002x1'></a>
<!-- l. 11 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>And obviously, any other parent version of </span><span class='ectt-0800'>buffer.py</span><span class='ecrm-0800'>.</span></p><a id='x1-1004x1'></a>
<!-- l. 21 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>I.e., the same signal that would be captured by our mic if I were using a headset.</span></p><a id='x1-4002x4'></a>
<!-- l. 50 --><p class='indent'>     <span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='ecrm-0800'>Or frame if we work in stereo</span></p><a id='x1-4005x4'></a>
<!-- l. 60 --><p class='indent'>     <span class='footnote-mark'><a href='#fn4x0-bk' id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><span class='ecrm-0800'>In a digital signal the sample index indicates the position of the sample in the sequence of
</span><span class='ecrm-0800'>samples. If we also know the sample-time, i.e., the cadency of the sampler, we can also compute at
</span><span class='ecrm-0800'>which time was taken a sample.</span></p><a id='x1-5002x5'></a>
<!-- l. 77 --><p class='indent'>     <span class='footnote-mark'><a href='#fn5x0-bk' id='fn5x0'><sup class='textsuperscript'>5</sup></a></span><span class='ecrm-0800'>This action is similar to the carried out by submarines when they user the sonar to perform
</span><span class='ecrm-0800'>echo-location, or by bats.</span></p><a id='x1-5006x5'></a>
<!-- l. 107 --><p class='indent'>     <span class='footnote-mark'><a href='#fn6x0-bk' id='fn6x0'><sup class='textsuperscript'>6</sup></a></span><span class='ecrm-0800'>The Fourier transform is an special case of the Laplace transform where</span> \(\sigma =0\) <span class='ecrm-0800'>in the complex
</span><span class='ecrm-0800'>(Laplace) domain represented by</span> \(s=\sigma +j\omega \) <span class='ecrm-0800'>frequencies. This simplification can be used for the
</span><span class='ecrm-0800'>characterization of our local-end audioset because it can be considered a FIR (Finite
</span><span class='ecrm-0800'>Impulse Response) system (in ausence of an audio signal, the echo always decays with the
</span><span class='ecrm-0800'>time).</span></p><a id='x1-6002x6'></a>
<!-- l. 121 --><p class='indent'>     <span class='footnote-mark'><a href='#fn7x0-bk' id='fn7x0'><sup class='textsuperscript'>7</sup></a></span><span class='ecrm-0800'>The number of coefficients.</span></p><a id='x1-6005x6'></a>
<!-- l. 146 --><p class='indent'>     <span class='footnote-mark'><a href='#fn8x0-bk' id='fn8x0'><sup class='textsuperscript'>8</sup></a></span><span class='ecrm-0800'>See </span><a class='url' href='https://www.youtube.com/watch?v=hc2Zj55j1zU'><span class='ectt-0800'>https://www.youtube.com/watch?v=hc2Zj55j1zU</span></a></p><a id='x1-6007x6'></a>
<!-- l. 148 --><p class='indent'>    <span class='footnote-mark'><a href='#fn9x0-bk' id='fn9x0'><sup class='textsuperscript'>9</sup></a></span><span class='ecrm-0800'>A neuron and a FIR filter are very similar.</span></p><a id='x1-6009x6'></a>
<!-- l. 168 --><p class='indent'>     <span class='footnote-mark'><a href='#fn10x0-bk' id='fn10x0'><sup class='textsuperscript'>10</sup></a></span><span class='ecrm-0800'>Again, see </span><a class='url' href='https://www.youtube.com/watch?v=hc2Zj55j1zU'><span class='ectt-0800'>https://www.youtube.com/watch?v=hc2Zj55j1zU</span></a><span class='ecrm-0800'>!</span></p><a id='x1-7008x11'></a>
<!-- l. 198 --><p class='noindent'><span class='footnote-mark'><a href='#fn11x0-bk' id='fn11x0'><sup class='textsuperscript'>11</sup></a></span><span class='ecrm-0800'>A 1-seconds cadence should be enought.</span></p>                                                                </div>
 
</body> 
</html>