\title{\href{https://www.ual.es/estudios/grados/presentacion/plandeestudios/asignatura/4015/40154321?idioma=zh_CN}{Tecnolog√≠as Multimedia} - Study Guide - Milestone 11: Inter-channel decorrelation in stereo audio signals}

\maketitle

\section{Description}

InterCom transmits a
\href{https://en.wikipedia.org/wiki/Stereophonic_sound}{stereo}
\href{https://en.wikipedia.org/wiki/Pulse-code_modulation}{PCM
  signals} with two channels (left and right). In most cases, both
channels are
\href{https://en.wikipedia.org/wiki/Binaural_recording}{highly
  correlated} (especially when the microphone is mono), which means
that we can find a more efficient representation than the provided by
the sound card.

From a mathematical point of view, this process can be seen as a
\href{https://en.wikipedia.org/wiki/Decorrelation}{decorrelation}
process. From a physical perspective, decorrelating implies \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{energy}
accumulation in some
\href{https://web.stanford.edu/class/ee398a/handouts/lectures/07-TransformCoding.pdf}{coefficients}~\cite{sayood2017introduction}. \href{https://en.wikipedia.org/wiki/Correlation_and_dependence}{Correlation}
is a term used in statistics for refering to the interdependency
between
two \href{https://en.wikipedia.org/wiki/Random_variable}{random
variables}, and it can be measured by the
\href{https://www.mathsisfun.com/data/correlation.html}{correlation
  coefficient}~\cite{thinkstats}. In the case of InterCom, we have two
  random variables, one for each channel~\cite{bosi2003intro}.

To perform this inter-channel decorrelation we can use the \href{https://en.wikipedia.org/wiki/Linear_map}{linear transform}
\begin{equation}
  w = Kx = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}x,
  \label{eq:forward_transform_matrix_form}
\end{equation}
or alternatively
\begin{equation}
  \begin{bmatrix}
    w_0 \\
    w_1
  \end{bmatrix}
  = 
  \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
  \begin{bmatrix}
    x_0 \\
    x_1
  \end{bmatrix},
  \label{eq:forward_transform_matrix_form2}
\end{equation}
where $x$ is a stereo frame, $K$ is the (forward) transform matrix and
$w=\begin{bmatrix} w_0 &
w_1\end{bmatrix}^{\text T}$ is the corresponding
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform#Example_in_image_processing}{decomposition}. In
this transform, the decomposition has two
\href{https://en.wikipedia.org/wiki/Sub-band_coding}{subbands} $w_0$
and $w_1$, and each subband has only one coefficient.

%where $x_0$ and $x_1$ are the samples of the stereo frame $x$.

In general (for all the linear transforms),
Eqs.~\ref{eq:forward_transform_matrix_form} and
\ref{eq:forward_transform_matrix_form2} can be also expressed as
\begin{equation}
  w_u = \sum_i K_{u,i}x_i,
  \label{eq:forward_transform_linear_combination_form}
\end{equation}
where $K_{u,i}$ denotes $i$-th element of the $u$-th row of the matrix
$K$.

A major difference between the transformed sequence $w$ and the
original sequence $x$ is that the characteristics of the elements of
$w$ are determined by their position within the
decomposition~\cite{sayood2017introduction}. Thus, as a consequence of
how the matrix has been defined, the subband $w_0$ represents (very
roughly) the low frequencies of $x$ and $w_1$ the high
frequencies. This can be also seen in this \href{}{notebook}.

Let's find the inverse
transform
\begin{equation}
  x = K^{-1}w.
  \label{eq:inverse_transform}
\end{equation}
From Eq. \ref{eq:forward_transform_matrix_form} we get that
\begin{equation}
  \begin{array}{rcl}
  w_0 & = & x_0 + x_1\\
  w_1 & = & x_0 - x_1.
  \end{array}
\end{equation}
By solving $x_0$ (adding) and $x_1$ (substracting) in
these equations, we obtain that
\begin{equation}
  \begin{array}{rcl}
  x_0 & = & \frac{1}{2}(w_0 + w_1)\\
  x_1 & = & \frac{1}{2}(w_0 - w_1),
  \end{array}
\end{equation}
that in matrix form becomes
\begin{equation}
  \begin{bmatrix}
    x_0 \\
    x_1
  \end{bmatrix}
  = \frac{1}{2}
  \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
  \begin{bmatrix}
    w_0 \\
    w_1
  \end{bmatrix}.
\end{equation}
Therefore,
\begin{equation}
  x = K^{-1}w = \frac{1}{2}K^{\text T}w = \frac{1}{2}Kw = \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}w.
  \label{eq:inverse_transform_matrix_form}
\end{equation}

Notice that (ignoring the $\frac{1}{2}$ scale factor) the inverse
transform is the transpose of the forward transform. This is a
characteristic of all
\href{https://en.wikipedia.org/wiki/Orthogonal_transformation}{orthogonal
  transforms}~\cite{sayood2017introduction}. For the transform that we
are analyzing, it also holds that $K^{\text T}=K$ because $K$ is
\href{https://en.wikipedia.org/wiki/Symmetric_matrix}{symmetric}.

Orthogonality is important for compression applications because the
correlation between subbands is 0, and therefore, the contributions of
the subbands to the reconstruction of the original signal are
independent\footnote{The distortion measure is such that the total
distortion is the sum of the distortion contribution of each
subband~\cite{sayood2017introduction}.}. Apart from checking that
$K^{-1}=K^{\text T}$, $K$ is orthogonal if the
\href{https://en.wikipedia.org/wiki/Inner_product_space}{inner
  product} (also called the
\href{https://en.wikipedia.org/wiki/Dot_product}{dot product} when we
work with
\href{https://en.wikipedia.org/wiki/Euclidean_space}{Euclidean
  spaces}) of the basis functions is $0$ between the different
basis. In our case $K_1=\begin{bmatrix}1 & 1\end{bmatrix}$ and
$K_2=\begin{bmatrix} 1 & -1\end{bmatrix}$~ is
\begin{equation}
  \langle K_1,K_2 \rangle =
  \langle \begin{bmatrix}
    1 & 1
  \end{bmatrix}
  ,
  \begin{bmatrix}
    1 & -1
  \end{bmatrix}
  \rangle =
  \begin{bmatrix}
    1 & 1
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    1 & -1
  \end{bmatrix}
   = 0.
\end{equation}
which means that $K_1$ and $K_2$ are linearly independent\footnote{We
cannot derive one from the other using the operations that define a
vector space.}, and therefore they form a basis
(set)~\cite{strang4linear}.

In our case, it also holds that
\begin{equation}
\begin{array}{l}
  \langle K_1,K_1 \rangle =
  \langle \begin{bmatrix}
    1 & 1
  \end{bmatrix}
  ,
  \begin{bmatrix}
    1 & 1
  \end{bmatrix}
  \rangle =
  \begin{bmatrix}
    1 & 1
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    1 & 1
  \end{bmatrix}
   = 2,
   \\
     \langle K_2,K_2 \rangle =
  \langle \begin{bmatrix}
    1 & -1
  \end{bmatrix}
  ,
  \begin{bmatrix}
    1 & -1
  \end{bmatrix}
  \rangle =
  \begin{bmatrix}
    1 & -1
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    1 & -1
  \end{bmatrix}
   = 2,
   \end{array}
\end{equation}
that implies that $K$ is not orthonormal, i.e., it is not energy
preserving~\cite{sayood2017introduction}. Another way of saying this
is that the Parseval's Theorem is not satisfied:
\begin{equation}
  \sum_i {w_i}^2 =
  \left[(x_0+x_1)\right]^2 + \left[(x_0-x_1)\right]^2 =
  (x_0^2+2x_0x_1+x_1^2) + (x_0^2-2x_0x_1+x_1^2) =
  2(x_0^2+x_1^2) =
  2\sum_i {x_i}^2.
  \label{eq:No_Parseval}
\end{equation}


The factor $2$ in the Eq.~\ref{eq:No_Parseval} is indicating that the
chain forward-inverse transform defined by $K$ has a gain of $2$ (the
output of the inverse transform will be equal to the input to the
forward transform multiplied by $2$, which physically means that we
are amplifying the audio signal by $2$). To avoid this, as the
Eq.~\ref{eq:inverse_transform_matrix_form} indicates, we must divide
each coefficient ($w_0$ and $w_1$) by $2$ after computing the forward
transform.

Finally, because the gain of each subband can\footnote{We could use
quantization steps $\Delta_1$ and $\Delta_2$ inversely proportional to
the gain of the subbands $w_1$ and $w_2$, removing less information
from the subband that has a higher gain.} be important in the
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization}
stage, let's analyze them. By definition, the gain of the subband
$w_i$ is the \href{https://en.wikipedia.org/wiki/Lp_space}{L$_2$ norm}
(named also
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{Euclidean
  distance} in multidimensional (higher than 3 dimenssions) spaces) of
the basis function $K_i$. Unfortunately, most of the transform are not
implemented using matrix-vector operations, but using faster
algorithms based on a sucession of lattice bufferflies with different
apertures. In general, we can determine $K_i$ simply by computing the
inverse transform of the decomposition $\begin{bmatrix} 0 & \cdots & 0
  & 1 & 0 & \cdots & 0 \end{bmatrix}$ where the $1$ value is in the
position $i$ of the decomposition $w_i$. In our example, we get that

\begin{equation}
  \begin{array}{l}
    \begin{bmatrix}
      1 & 1 \\
      1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      0
    \end{bmatrix}
    =
    \begin{bmatrix}
      1 & 1
    \end{bmatrix} = K_1,
    \\
    \begin{bmatrix}
      1 & 1 \\
      1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      0 \\
      1
    \end{bmatrix}
    =
    \begin{bmatrix}
      1 & -1
    \end{bmatrix} = K_2.
  \end{array}
\end{equation}

Next, we compute the L$_2$ norm\footnote{Notice that the L$_2$ norm is
not the only measurement for the signal energy, but it is the most
common.} of the basis functions as

\begin{equation}
  \begin{array}{l}
    \left\| K_1 \right\|_2 := \sqrt{\langle \begin{bmatrix}1 & 1\end{bmatrix}, \begin{bmatrix}1 & 1\end{bmatrix} \rangle} = \sqrt{\begin{bmatrix}1 & 1\end{bmatrix}\cdot \begin{bmatrix}1 & 1\end{bmatrix}} = \sqrt{2},\\
    \left\| K_2 \right\|_2 := \sqrt{\langle \begin{bmatrix}1 & -1\end{bmatrix}, \begin{bmatrix}1 & -1\end{bmatrix} \rangle} = \sqrt{\begin{bmatrix}1 & -1\end{bmatrix}\cdot \begin{bmatrix}1 & -1\end{bmatrix}} = \sqrt{2},
  \end{array}
\end{equation}

resulting that both subbands $w_1$ and $w_2$ have the same gain
($\sqrt{2}$). This result tell us that both subbands should use the
same quantization step ($\Delta_1=\Delta_2$). Unfortunately, this
estatement is only true if the entropy coding stage generates the same
number of bits for both subbands, something that rarely happens
because we are compressing the coefficients considering the whole
chunk (we are exploiting the statistical correlation between the
sequence of coefficients generated by all the frames of a chunk). In
general, the amount of information of provided by the sequence of
$w_i$ values is different, and therefore, the RD (Rate Distortion)
curve of each sequence is distint.

The standard solution for this problem is to use $\Delta_i$ values
that select those RD points that provides the same RD
slopes~\cite{vetterli2014foundations, sayood2017introduction}. A
discrete RD curve is defined by the Pareto front form by the RD
points. A RD point is defined as a pair or $(r,d)$ values where $r$
represents a bit-rate (typically expressed in bits/sample) and $d$
represents a distortion (that uses to be the RMSE when we use the
L$_2$ norm to measure distances). Therefore, to find the two RD curves
for the current chunk, we should apply the stereo transform, apply a
set of $\Delta_i$ values to each subband, and compress the resulting
quantization indexes. This would find the $r$ values of our RD
curve. Then, decompress, dequantize and find the distortion for the
chunk. This would find the $d$ values. Finally, with this RD curve, we
should select the $\Delta_i$ values that provides the same slopes for
both subbands.

Obviously, we can not use the previous algoritm for computing the RD
curves in a real-time application such as InterCom.\footnote{The
amount of computational resources would increase significatively.} We
need to make some suppositions in order to reduce the computational
cost of finding the RD curves. The first of our suppositions is that
between (temporally) adjacent chunks the RD curves are going to be
similar. Therefore, we can build the RD curve for the current chunk by
using the RD points generated\footnote{Each chunk is quantized and
compressed, so, we only need to compute the distortion to have the RD
point used for this chunk.} by the compression of previously processed
chunks. The second supposition is that we can estimate the average
slope of the complete RD curve by using only 2 RD points. Using this
information, we will try to use, for the current chunk, a pair of
$\Delta_i$ quantization steps that produce two RD curves (one curve
per subband) with the same average slope for the current chunk.

============

 This
property is satisfied by all orthogonal
transforms~\cite{sayood2017introduction}.
\begin{equation}
  KK^T=K^TK=2I,
  \label{eq:ortho_matrix_test}
\end{equation}
where $I$ is the identity matrix.

The 

Eq.~\ref{eq:ortho_matrix_test} is showing also that the gain of
coefficient is the same: $2$. This fact can be also verified comparing
the energy of the reconstruction provided by each coefficient:

\begin{equation*}
  \begin{array}{l}
    \begin{bmatrix}
      1 & -1 \\
      1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      0
    \end{bmatrix}
    =
    \begin{bmatrix}
      1 & 1
    \end{bmatrix} = K_1
    \\
    \begin{bmatrix}
      1 & -1 \\
      1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      0 \\
      1
    \end{bmatrix}
    =
    \begin{bmatrix}
      -1 & 1
    \end{bmatrix} = K_2
    \\
    \left\| \begin{bmatrix}1 & 1\end{bmatrix} \right\|_2 := \sqrt{\langle \begin{bmatrix}1 & 1\end{bmatrix}, \begin{bmatrix}1 & 1\end{bmatrix} \rangle} = \sqrt{\begin{bmatrix}1 & 1\end{bmatrix}\cdot \begin{bmatrix}1 & 1\end{bmatrix}} = \sqrt{2}\\
    \left\| \begin{bmatrix}-1 & 1\end{bmatrix} \right\|_2 := \sqrt{\langle \begin{bmatrix}-1 & 1\end{bmatrix}, \begin{bmatrix}-1 & 1\end{bmatrix} \rangle} = \sqrt{\begin{bmatrix}-1 & 1\end{bmatrix}\cdot \begin{bmatrix}-1 & 1\end{bmatrix}} = \sqrt{2},
  \end{array}
\end{equation*}
where $\left\|v \right\|_2$ denotes the L$_2$ norm of the vector $v$.

This information, the gain of each coefficient, is relevant for the
quantization stage, which ideally, should allow to compress each pair
of coefficients with the same rate/distortion slope. To achieve this

Because both coefficients have the same gain, the number of bits assigned by the quantizer should be the same

As it can be seen in this, the gain of each subband (and each coefficient, because we have only one coefficient per subband)

although both ``subbands''\footnote{In a effort of extrapolate this
discussion to a transform case where each subband has more than only
one coefficient, we have called to $w[0]$ the low-pass subband and
$w[1]$ the high-pass subband. However, notice that each subband has
only one coefficient!} $w[0]$ and $w[1]$ have the same gain
($\frac{1}{2}$, and therefore the same ``importance'' for a
future
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization}
of $w$, i.e., the same quantization step can be used for the $w[0]$
and $w[1]$ coefficients).

Notice that this procedure allows to find the basis vectors ($K_1$ and
$K_2$ in our case) from an implementation of the inverse
tranform.\footnote{Normally, the transform is not implemented as a
matrix-vector multiplication and therefore, the basis vectors are not
directly available.}

The matrix $K$ proposed is similar to the $2\times 2$ KLT
\href{http://fourier.eng.hmc.edu/e161/lectures/klt/node3.html}{(Karhunen-Lo\`eve
  Transform)}, the
\href{http://wavelets.pybytes.com/wavelet/haar/}{Haar
  Transform}~\cite{vetterli1995wavelets} and the $2\times 2$
\href{https://en.wikipedia.org/wiki/Hadamard_transform}{Discrete
  Walsh-Hadamard Transform}~\cite{sayood2017introduction}).  The
described transform is also similar to the so called
\href{https://en.wikipedia.org/wiki/Joint_encoding#M/S_stereo_coding}{M/S
  stereo coding}, but in our case, the divisi√≥n by 2 is carried on the
forward transform, instead of the inverse (backward) transform. This
is done to ensure that the transform can be computed
\href{https://en.wikipedia.org/wiki/In-place_algorithm}{\emph{in-place}}~\cite{2006.sweldens}. Such
implementation is:

\begin{pseudocode}{Inter-channel\_decorrelation}{~}
  \PROCEDURE{analyze}{\text{frame}}
  \BEGIN
    \text{frame}[1] -= \text{frame}[0] \\
    \text{frame}[0] += (\text{frame}[1] / 2) \\
    \text{frame}[1] /= 2
  \END
  \ENDPROCEDURE
  \PROCEDURE{synthesize}{\text{frame}}
  \BEGIN
    \text{frame}[1] *= 2 \\
    \text{frame}[0] -= (\text{frame}[1] / 2) \\
    \text{frame}[1] += \text{frame}[0]
  \END
  \ENDPROCEDURE
\end{pseudocode}

where $\text{a}~\mathtt{OPER}= \text{b}$ is a shorter representation of the operation
$\text{a} = \text{a}~\mathtt{OPER}~\text{b}$.

\section{What you have to do?}

\begin{enumerate}
\item In a module named stereo.py, inherit the class
  Quantization and create a class named Stereo\_decorrelation.
\item Override the methods pack() and unpack(). In
  pack() perform the procedure analyze() previously
  described, and in unpack() the
  synthesize(). These procedures should be applied to
  all the frames of a chunk using \href{https://www.oreilly.com/library/view/python-for-data/9781449323592/ch04.html}{vectorized
    operations}.
\item Has been the
  \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio} improved (on
  \href{https://en.wikipedia.org/wiki/Average}{average})? How much?
\end{enumerate}

\section{Timming}

You should reach this milestone at most one week.

\section{Deliverables}

The module stereo.py. Store it at the
\href{https://github.com/Tecnologias-multimedia/intercom}{root
  directory} of your InterCom's repo.

\section{Resources}

\bibliography{maths,data-compression,DWT,audio-coding}
