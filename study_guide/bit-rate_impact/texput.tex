\input{../definitions}
\title{\TM - Study Guide - Milestone 7: Impact of the Bit-rate}

\maketitle

\section{Description}

Along with the latency and its variation (jitter), another main aspect
to consider about the
\href{https://en.wikipedia.org/wiki/Telecommunications_link}{transmission
  link} used in an InterCom session is the
\href{https://en.wikipedia.org/wiki/Bit_rate}{transmission bit-rate}
(measured in bits per second or a multiple of this transmission
capacity) that it can provide~\cite{Forouzan,Tanenbaum}. This bit-rate
depend on the maximum
\href{https://en.wikipedia.org/wiki/Bandwidth_(computing)}{capacity}
(a characteristic closely related with the available bandwidth) and
its \href{https://en.wikipedia.org/wiki/Network_congestion}{congestion
  level} (that basically depends on the load) of the link. In general,
we can suppose that the capacity is constant along the time (the
bandwidth provided by the link does not vary with the time). On the
contrary, the congestion level is time-variying and quite
unpredictable, because it depends on the behaviour of the network
users.

In this milestone we are going to measure the impact of the
transmission bit-rate on the
\href{https://en.wikipedia.org/wiki/Quality_of_experience}{QoE}\footnote{Follow
  the same rubric than in the case of the measurement of the impact of
  the latency.} provided by the minimal implementation of InterCom to
the users. Similarly to the procedure used for measuring the impact of
the latency and the jitter, we will use
\href{https://man7.org/linux/man-pages/man8/tc.8.html}{\texttt{tc}}~\cite{bert2012lartc}
to control the amount of data that an InterCom instance will be
allowed to send in a local environment, with the aim of simulating a
real running context. Notice that this limitation will also affect to
the loss of chunks because if the transmission bit-rate is smaller
than the audio bit-rate, sooner of later the link will discard those
chunk that can not be buffered in the retranmission nodes (routers and
switches)\footnote{Notice that, in this case, we would be at least
  contributing, if not causing, the link congestion.}.

\section{What you have to do?}

\subsection{Estimate the bit-rate in an Internet link}

Usually, we need to use a tool such as \href{https://iperf.fr/}{iPerf}
to
\href{https://en.wikipedia.org/wiki/Measuring_network_throughput}{measure
  the transmission bit-rate} between two end-points in the
Internet. This proceduce implies that we must have access to both
hosts to install and run this program, privileges that are not always
available.

Alternatively, we can estimate the
\href{https://en.wikipedia.org/wiki/Throughput}{link throughput} using
\href{https://github.com/torvalds/linux/blob/master/net/ipv4/ping.c}{\texttt{ping}}
(see the previous milestone). The reader should understand that
\texttt{ping} has been designed for measure latencies, not bit-rates,
and that for this reason, we will be able to estimate throughputs
rougtly.

The transmission bitrate avaiable between two directly reachable IP
devices can be estimated as
\begin{equation}
  b=\frac{B}{t_t},
  \label{eq:b}
\end{equation}
where $B$ the number of bits sent in a \texttt{ping}'s payload. Now,
if we revisite the \texttt{ping}'s timeline of the previous milestone
and the Equation~(2), we can see that
\begin{equation}
  t_t = \frac{\text{RTT}-2t_p}{2}.
  \label{eq:tt}
\end{equation}
Therefore, supposing that $t_p$ is constant between pings and using
the average RTT, $b$ can be considered as a good estimation of the
near future.

So, for determining $b$ we can:
\begin{enumerate}
\item Run \texttt{ping} using the smallest possible payload. In this
  situation we can suppose that
  \begin{equation}
    t_p \approx \frac{\text{RTT}}{2}.
    \label{eq:tp}
  \end{equation}
  You should use the smaller RTT of a \texttt{ping} session for such
  approximation because the real link latency is a constant and the
  congestion of the link always increases it. The minimal payload for
  a \texttt{ping} message can be 0, but depending on the
  implementation of \texttt{ping} you may be forzed to use a larger
  value.
\item Run \texttt{ping} using the largest possible payload to achieve
  that most part of the average RTT provided by \texttt{ping}
  corresponds to $t_t$. This is necessary because the accuracy of the
  cronometer used by \texttt{ping}, in general, is not high enough to
  measure $t_t$.
\item Apply the Equation~\ref{eq:tt} to find $t_t$ as a function of
  the average RTT and the minimal $t_p$.
\item Finally, use the Equation~\ref{eq:b} to determine $b$.
\end{enumerate}

In order to measure your bandwith with different \texttt{ping}
servers, try to compute the transmission bit-rate:
\begin{enumerate}
\item In \texttt{localhost}.
\item Between two hosts in your local network (your host and your
  router, for example). Notice that in this case, you should not
  obtain a bit-rate higher than the capacity of your network adapter.
\item Between your host and a public host of the Internet. Try
  \texttt{www.meter.net} for example.
\end{enumerate}

If the devices cannot \texttt{ping} each other, and supposing that
there is only one point in the communication link that is filtering
the ICMP Echo Request traffic in each direction, then we can apply the
same technique that was used in the previous milestone, which
basically consists in compute the total RTT as the sum of the RTT from
the devices that we want to connect (where the InterCom is it supposed
to be run) to the filter (possiblely, one of your home routers).

\subsection{Simulate the link}

With the bit-rate vaule computed from the previous section, simulate
such link locally with:

\begin{lstlisting}{language=bash}
  sudo tc qdisc add dev lo root handle 1: tbf rate <bit-rate_in_kbps>kbit burst 32kbit limit 32kbit
  sudo tc qdisc add dev lo root parent 1:1 handle 10: netem delay <average_delay_in_miliseconds>ms <maximum_average_deviation_in_miliseconds>ms <Pearson_correlation_coefficient_expressed_as_a_percentage>% distribution <uniform|normal|pareto|paretonormal>
\end{lstlisting}

Example:

\begin{enumerate}
\item Check the current configuration:
  
  \begin{lstlisting}{language=bash}
    tc qdisc show dev lo
  \end{lstlisting}
  
  The output should be something like:
  
  \begin{lstlisting}{language=bash}
    qdisc noqueue 0: root refcnt 2
  \end{lstlisting}
  
\item Define the rule for the bit-rate control.
  
  \begin{lstlisting}{language=bash}
    sudo tc qdisc add dev lo root handle 1: tbf rate 200kbit burst 32kbit limit 32kbit
  \end{lstlisting}
  
\item Define the rule for the latency control (see previous milestone).
  
  \begin{lstlisting}{language=bash}
    sudo tc qdisc add dev lo parent 1:1 handle 10: netem delay 100ms 10ms 25% distribution normal
  \end{lstlisting}
  
\item After adding these rules, this should be the configuration:
  
  \begin{lstlisting}{language=bash}
    tc qdisc show dev lo
  \end{lstlisting}
  
  The output should be something like:
  
  \begin{lstlisting}{language=bash}
    qdisc tbf 1: root refcnt 2 rate 200Kbit burst 4Kb lat 0us 
    qdisc netem 10: parent 1:1 limit 1000 delay 100ms  10ms 25%
  \end{lstlisting}

\end{enumerate}

Remember to delete the rules after their are unnecessary. Example:

\begin{lstlisting}{language=bash}
  sudo tc qdisc delete dev lo parent 1:1 handle 10: netem delay 100ms 10ms 25% distribution normal
  sudo tc qdisc delete dev lo root handle 1: tbf rate 200kbit burst 32kbit limit 32kbit
\end{lstlisting}

\section{Timming}

Please, finish this milestone in one week.

\section{Deliverables}

A report showing your results.

\section{Resources}

\bibliography{networking}
