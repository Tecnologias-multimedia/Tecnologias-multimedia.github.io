\title{\href{https://www.ual.es/estudios/grados/presentacion/plandeestudios/asignatura/4015/40154321?idioma=zh_CN}{Tecnolog√≠as Multimedia} - Study Guide - Milestone 5: Minimal InterCom}

\maketitle

\section{Description}

\subsection{Sequential model}

InterCom can be divided into 6 steps:

\begin{pseudocode}{Sequential\_InterCom}{~}
  \BEGIN
    \mathtt{recorded\_chunk} \GETS \mathtt{record()}\\
    \mathtt{outgoing\_packet} \GETS \mathtt{pack(recorded\_chunk)}\\
    \mathtt{send(outgoing\_packet)}\\
    \mathtt{incoming\_packet} \GETS \mathtt{receive()}\\
    \mathtt{chunk\_to\_play} \GETS \mathtt{unpack(incoming\_packet)}\\
    \mathtt{play(chunk\_to\_play)}
  \END
\end{pseudocode}

Where:

\begin{enumerate}
\item $\mathtt{record()}$ captures a chunk of frames. In general, this
  is a \href{https://en.wikipedia.org/wiki/I/O_bound}{I/O-bound}
  \href{https://python-sounddevice.readthedocs.io/en/0.4.0/api/streams.html#sounddevice.Stream.read}{blocking
    operation}, depending or not on the availability of frames in the
  ADC. This can be checked in
  \href{https://raw.githubusercontent.com/Tecnologias-multimedia/intercom/master/test/sounddevice/wire4.py}{wire4.py},
  where the $\mathtt{read()}$ method always returns inmediately
  because only the available frames are returned in each call (whose
  variability depends on the instant of time in which this method is
  call).
\item $\mathtt{pack(chunk)}$ process the chunk to create a
  \href{https://en.wikipedia.org/wiki/Network_packet}{packet} (or a
  sequence of packets), a structure that can be transmitted through
  the Internet using the
  \href{https://en.wikipedia.org/wiki/Datagram}{Datagram} Model. In
  general, this is a
  (\href{https://en.wikipedia.org/wiki/CPU-bound}{CPU-bounded}
  (CPU-intensive) operation, and therefore, ``blocks'' the execution
  main thread until the packing has not been finithed.
\item $\mathtt{send(packet)}$, sends the packet to the
  interlocutor's InterCom. When datagrams are used, this step is
  non-blocking neither CPU-bounding because the
  CPU usage is very low.
\item $\mathtt{receive()}$, waits (blocking the task) for a incoming
  packet, and therefore, this operation is IO-bound. However, most
  socket APIs offeer a
  \href{https://docs.python.org/3.8/library/socket.html#socket.socket.setblocking}{non-blocking
    behaviour} where when a packet is not available in the kernel's
  buffer associanted to the corresponding socket, some kind of
  exception is generated and in this case, it is resposabability of
  the programmer to generate a ``alternative'' chunk in our case (for
  example, a chunk filled with zeros will not produce any sound).
\item $\mathtt{unpack(packet)}$ is, as $\mathtt{pack(chunk)}$, a
  CPU-intensive step that transforms a packet (or several packets)
  into a chunk of audio.
\item $\mathtt{play(chunk)}$ renders the chunk, reproducing it. In
  general, this is an I/O-bound
  \href{https://python-sounddevice.readthedocs.io/en/0.4.0/api/streams.html#sounddevice.Stream.write}{blocking}
  action. However, if $\mathtt{play()}$ is called at the same pace
  than $\mathtt{record()}$ and the record and play parameters are
  exactely the same (as happens in our model), the playing of the
  chunk will never blocks the main thread because the time that the
  $\mathtt{play()}$ method needs to complete is exactly the same time
  that the $\mathtt{record()}$ method also requires.
\end{enumerate}
  
Notice that the execution of these steps, in this order, in two
different InterCom instances has at least two problems:
\begin{enumerate}
\item If a packet is lost, a deadlock is generated, because the
  $\mathtt{receive()}$ never returns.
\item If the time required by $\mathtt{pack(chunk)}$ and
  $\mathtt{unpack(packet)}$ is largest than the chunk-time, a glich
  will be generated.
\end{enumerate}

\subsection{Concurrent model}

The \href{https://en.wikipedia.org/wiki/Parallel_computing}{parallel}
model supposes that there is at least two Processing Units (or PUs,
typically
\href{https://en.wikipedia.org/wiki/Multi-core_processor}{cores}),
that are able to work in parallel. Thus, the sequential model is
divided into two
\href{https://en.wikipedia.org/wiki/Concurrency_(computer_science)}{concurrent}
tasks that should run in parallel.

\begin{pseudocode}{Concurrent\_InterCom}{~}
  \PROCEDURE{Record\_Pack\_and\_Send}{~}
  \BEGIN
    \mathtt{chunk} \GETS \mathtt{record()}\\
    \mathtt{packet} \GETS \mathtt{pack(chunk)}\\
    \mathtt{send(packet)}
  \END
  \ENDPROCEDURE
  \PROCEDURE{Receive\_Unpack\_and\_Play}{~}
  \BEGIN
    \mathtt{packet} \GETS \mathtt{receive()}\\
    \mathtt{chunk} \GETS \mathtt{unpack(packet)}\\
    \mathtt{play(chunk)}
  \END
  \ENDPROCEDURE
\end{pseudocode}

In this case, the loss of a chunk does not generates a deadlock
between the concurrent tasks, because even if the
$\mathrm{Receive\_Unpack\_Play}$ is blocked waiting for a block in the
$\mathtt{receive()}$, the $\mathtt{Record\_Pack\_and\_Send}$ task will
continue sendind chunks.

Unfortunatelly, gliches can still happen in this model if the packets
are lost or delayed, or if $\mathtt{pack(chunk)}$ or if
$\mathtt{unpack(packet)}$ needs more than a chunk-time to be
completed.

Most of the socket APIs offeer a version of 

\subsection{Timer-based model}

In this model, the task dedicated to record and play the chunks of
audio is called periodically (probably, using some timer provided by
the sound hardware). This procedure guarantees a smoother audio-IO
when constant chunk sizes are used because the timer interruption
matches with the instant of time in which the $\mathtt{record()}$ and
the $\mathtt{play()}$ methods need to be run to guarantee that are
going to handle a chunk of audio of a known size without blocking.


\begin{pseudocode}{Timer-based\_InterCom}{~}
  \PROCEDURE{Record\_and\_Play}{\mathtt{recorded\_chunk}, \mathtt{chunk\_to\_play}, \mathtt{chunk\_size}}
  \BEGIN
    \mathtt{recorded\_chunk} \GETS \mathtt{record(chunk\_size)}\\
    \mathtt{(recorded\_chunk)}\\
    \mathtt{
    \mathtt{send(outgoing\_packet)}\\
    \mathtt{play}(\mathtt{chunk\_to\_play})\\
  \END
  \ENDPROCEDURE
\end{pseudocode}

where:
\begin{enumerate}
\item $\mathtt{indata}$ points to the last chunk of audio recorded.
\item $\mathtt{outdata}$ points to the chunk of audio that is going to be played.
\item $\mathtt{chunk\_size}$ is the number of frames processed in each iteration of the callback.
\end{enumerate}

The advantage of this alternative compared to the Concurrent Model is
that we can control the size of the buffers, which in the end controls the latency.

\section{What you have to do?}

\begin{enumerate}

\item
  
\end{enumerate}

\section{Timming}

You should reach this milestone at most in one week.

\section{Deliverables}

\section{Resources}

\bibliography{python}
