\title{\href{https://www.ual.es/estudios/grados/presentacion/plandeestudios/asignatura/4015/40154321?idioma=zh_CN}{Tecnologías Multimedia} - Study Guide - Milestone 12: Temporal decorrelation in audio signals}

\maketitle

\section{Description}

After removing the spatial (stereo) redundancy in the InterCom
application, the next natural step in its development is the removal
of the temporal redundancy that can be found inside of each
subband\footnote{Notice that, https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_bankshttps://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_bankshttps://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_bankshttps://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banksbeacuse the MST and the DWT used
  transform in this milestone are both lineal transforms, the order in
  which the transforms are applied is irrelevant. For this reason, we
  could also have used ``inside of each channel'', and then, remote
  the spatial redundancy}. As it can be seen in this notebook, most
audio signals show *patterns* of samples that tends to repeat,
especially locally. Another clear source of temporal redundancy is
that the neighbor audio samples usually show similar amplitude values.

There are several techniques that can be used for removing the
temporal redundancy from a sequence of audio. Maybe, the most
straightforward one is to use
\href{https://en.wikipedia.org/wiki/Differential_pulse-code_modulation}{Differential
  Pulse Code Modulation
  (DPCM)~\cite{sayood2017introduction}}. However, there are more
efficient decorrelation algorithms based on
\href{https://en.wikipedia.org/wiki/Transform_coding}{transform
  coding}, such the used in the previous milestone.

Transform coding is based on the idea that we can decompose (we can
generate a decomposition from) the input signal into a set of
subbands, and if the used filters\footnote{Basis functions or vectors
  in a more pure mathematical context.} are the adecuate for removing
the temporal redundancy, we can achieve a high transform coding gain,
accumulating most of the signal energy (information) in a small number
of subbands. When this is true, a quantization process of the
coefficients of the subbands will remove basically the least
significant information, allowing\footnote{Notice that if we use, for
  example, a dead-zone quantizer and most of the coefficients are
  close to zero, the quantizer will generate a high number of zero
  quantization indexes and therefore, a high number of dequantized
  coefficients will be equal to zero.} better compression ratios than
those in which we apply the same quantization process to the original
samples.

Let's suppose that our analysis transform matrix $K$ has only two
(digital) filters with filter coefficients (or taps) $K_0$ and $K_1$
(see the previous milestone), and that we apply them to the input
signal $x$. In the context of
\href{https://en.wikipedia.org/wiki/Sub-band_coding}{subband coding},
$K$ is describing a 
\href{https://en.wikipedia.org/wiki/Filter_bank}{Filter Bank (FB)} and we are descomposing $x$ into
two subbands $w_0$ and $w_1$ (see the Figure~\ref{fig:PRFB}, and this
\href{}{notebook}). Remember, however, that in the context of this
milestone, $x$ is a sequence of MST coefficients that show some degree
of temporal correlation, not a simple pair of samples (a frame).

Supposing that (the taps) $K_0$ applied to $x$ removes basically
high-frequency information and $K_1$ removes low-frequency
information, and that the
\href{https://en.wikipedia.org/wiki/Filter_(signal_processing)#The_transfer_function}{transfer
  function} of both filters
\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{are
  the inverse of each other}, 

%\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{Perfect
%  Reconstruction Filter Bank (PRFB)} and we are descomposing $x$ into

For removing the temporal redundancy we will use the Discrete Wavelet
Transform (DWT). DWT has been selected because it is
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform#Time_complexity}{fast
  ($O(N)$)}. and it is able to represent efficienty
\href{https://en.wikipedia.org/wiki/Transient_(oscillation)}{transient}
signals, which can happen frequencly in audio. Moreover, although we
are not going to take advantage of the following characteristic (for
now), one of the most interesting features of the DWT is that it can
used to find easely a
\href{https://en.wikipedia.org/wiki/Multiresolution_analysis}{multiresolution
  representation} of the signal.

the autocorrelation of a signal can show how 


\begin{comment}

  The Figure~\ref{fig:transform_coding} shows the stages that are
  tipycally involved in a transform-based signal compression system.

\begin{figure}
  \begin{center}
\begin{verbatim}
   s   +---+   w    +---+   k    +---+    c
 ----->| T |------->| Q |------->| E |-----------+
  (s)  +---+  (s)   +---+  (~s)  +---+   (~s)    |
samples   coefficients   indexes      code-words ~
                                                 :
                                                 ~
   ~s  +---+    w   +---+   k    +---+           |
 <-----| t |<-------| q |<-------| D |<----------+
  (~s) +---+  (~s)  +---+  (~s)  +---+
approx.    quantized     indexes
samples   coefficients
\end{verbatim}                
  \end{center}
  \caption{Common data-flow used un Transform Coding. $s$ represents
    the signal to compress, $\tilde{s}$ the lossy version of the
    reconstructed signal, $T$ the (forward) transform (which takes blocks of
    samples) producing blocks of coefficients $w$, $Q$ the scalar
    quantization stage (which takes single coefficients) producing
    quantization indexes $k$, $E$ the entropy encoder (which in our
    case (DEFLATE) works with blocks of coefficients) producing
    code-words $c$, $D$ the entropy decoder, $q$ the decuantization
    stage, and $t$ the inverse (or backward) transform. PCM stands for Puse Code
    Modulation and DEFLATE is the technique used to find a compact
    representation of the quantized coefficients.}
  \label{fig:transform_coding}
\end{figure}
\end{comment}

There exist docens of suitable transform used in TC. They are
characterized by the
\href{https://en.wikipedia.org/wiki/Orthogonality}{orthogonality}
property, which basically means that the output coefficients are going
to be decorrelated because the functions (or vectors, depending on the
terminology) that describe the transform are
\href{https://en.wikipedia.org/wiki/Orthogonal_functions}{orthogonal}
and therefore, form a
\href{https://en.wikipedia.org/wiki/Basis_(linear_algebra)}{basis
  (set)} with which is possible to describe any signal in terms
(usually as a linear combination) of the (basis) functions of such
basis, i.e., are able to describe any signal in the vector space
spanned by the basis, using an unique description (set of
coefficients)~\cite{}.

Wavelets and Wavelet Transforms
% https://cnx.org/contents/EQurkhlI@6.9:9Qg8uP5e@5/Introduction-to-Wavelets
% https://cnx.org/exports/110bab92-1948-4958-b1c7-8fc0926c392c@6.9.pdf/wavelets-and-wavelet-transforms-6.9.pdf

Other major family of transform used in TC forms bi-orthogonal basis
(sets). In this case,

In this milestone we will use an orghogonal wavelet transform. This
makes that the quantization of the coefficients can be uniform, i.e.,
we can use the same quantization step for all the coefficients.

In the transform described in the previous milestone, the concept of subband is degradated because we have only one coefficient per subband.

The coefficient $w[0]$ is called the DC (Direct Current) coefficient, and the rest of coefficients are called AC (Alternating Current) coefficients.

\href{https://en.wikipedia.org/wiki/Correlation_and_dependence}{Correlation}
is a term used in statistics which refer to the interdependency
between two \href{https://en.wikipedia.org/wiki/Random_variable}{random
  variables}. It can be measured by the
\href{https://www.mathsisfun.com/data/correlation.html}{correlation
  coefficient}~\cite{thinkstats}.

In the case of InterCom, the random variables are the two channels
(left $L$ and right $R$) of the
\href{https://en.wikipedia.org/wiki/Stereophonic_sound}{stereo
  \href{https://en.wikipedia.org/wiki/Pulse-code_modulation}{PCM}
  signal}~\cite{bosi2003intro}. In most cases, both channels are going
to be \href{https://en.wikipedia.org/wiki/Binaural_recording}{highly
  correlated} (especially if the microphone is mono), which means that
we can represent one of them (for example, the $R$ channel) with
respect to the other (the $L$ channel). From a mathematical point of
view, this process can be seen as a
\href{https://en.wikipedia.org/wiki/Decorrelation}{decorrelation}
process. From a physical perspective, decorrelating implies energy
accumulation in a few coefficients~\cite{sayood2017introduction}.

To perform this inter-channel decorrelation, we can use an
\href{https://en.wikipedia.org/wiki/Orthogonal_transformation}{orthogonal}\footnote{Orthogonality
  of the transform is a important property because the correlation
  between the coefficients (the output of the transform) is 0.}
transform, that in the case of decorrelating a stereo signal is
\begin{equation}
  y = Kx = \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}x,
\end{equation}
where $x$ represents a frame (a tuple of L and R samples, $x[0]$ and
$x[1]$), $K$ is the $2\times 2$ KLT
\href{http://fourier.eng.hmc.edu/e161/lectures/klt/node3.html}{(Karhunen-Lo\`eve
  Transform)} matrix multiplied by $1/\sqrt{2}$ (which is closely
related to the \href{http://wavelets.pybytes.com/wavelet/haar/}{Haar
  transform}~\cite{vetterli1995wavelets}), and $y$ represents the
transform coefficients (in our case, a couple of coefficients $y[0]$
with the \href{https://en.wikipedia.org/wiki/Arithmetic_mean}{mean}
and $y[1]$ with the difference of the samples). Notice that this
transform is not
\href{https://en.wikipedia.org/wiki/Orthonormal_basis}{orthonormal}
(energy preserving in the transform domain) because
\begin{equation}
  \sum y[i]^2 = \frac{1}{\sqrt{2}}\sum x[i]^2,
\end{equation}
although both subbands $y[0]$ and $y[1]$ have the same gain
($\frac{1}{\sqrt{2}}$, and therefore the same ``importance'' for a
future
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization}
of $y$). The described transform is similar to the so called
\href{https://en.wikipedia.org/wiki/Joint_encoding#M/S_stereo_coding}{M/S
  stereo coding}, but in our case, the división by 2 is carried on
the forward transform, instead of the backward (inverse) transform.

This transform can be implemented
\href{https://en.wikipedia.org/wiki/In-place_algorithm}{\emph{in-place}}
using the following algorithm:

\begin{pseudocode}{Inter-channel\_decorrelation}{~}
  \PROCEDURE{analyze}{\text{frame}}
  \BEGIN
    \text{frame}[0] -= \text{frame}[1] \\
    \text{frame}[1] += (\text{frame}[0] / 2) \\
    \text{frame}[0] /= 2
  \END
  \ENDPROCEDURE
  \PROCEDURE{synthesize}{\text{frame}}
  \BEGIN
    \text{frame}[0] *= 2 \\
    \text{frame}[1] -= (\text{frame}[0] / 2) \\
    \text{frame}[0] += \text{frame}[1]
  \END
  \ENDPROCEDURE
\end{pseudocode}

where $\text{a}~\mathtt{OPER}= \text{b}$ is a shorter representation of the operation
$\text{a} = \text{a}~\mathtt{OPER}~\text{b}$. Notice that this type of in-place computations
are commonly used in the implementation of DWTs
(\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{Dicrete
  Wavelet Transform}s) using
\href{https://cm-bell-labs.github.io/who/wim/papers/athome/athome.pdf}{the
  Lifting Scheme}~\cite{2006.sweldens}.

\section{What you have to do?}

\begin{enumerate}
\item In a module named stereo.py, inherit the class
  Quantization and create a class named Stereo\_decorrelation.
\item Override the methods pack() and unpack(). In
  pack() perform the procedure analyze() previously
  described, and in unpack() the
  synthesize(). These procedures should be applied to
  all the frames of a chunk using \href{https://www.oreilly.com/library/view/python-for-data/9781449323592/ch04.html}{vectorized
    operations}.
\item Has been the
  \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio} improved (on
  \href{https://en.wikipedia.org/wiki/Average}{average})? How much?
\end{enumerate}

\section{Timming}

You should reach this milestone at most one week.

\section{Deliverables}

The module stereo.py. Store it at the
\href{https://github.com/Tecnologias-multimedia/intercom}{root
  directory} of your InterCom's repo.

\section{Resources}

\bibliography{maths,data-compression,DWT,audio-coding}
