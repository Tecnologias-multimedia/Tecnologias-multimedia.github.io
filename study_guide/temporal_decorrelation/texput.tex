\title{\href{https://www.ual.es/estudios/grados/presentacion/plandeestudios/asignatura/4015/40154321?idioma=zh_CN}{Tecnolog√≠as Multimedia} - Study Guide - Milestone 12: Temporal decorrelation in audio signals}

\maketitle

\section{Description}

% About the redundancy
After removing the spatial (stereo) redundancy in the previous
milestone, the next natural step in its development is the removal of
the temporal redundancy that can be found inside of each
subband\footnote{Notice that, beacuse the MST and the transform used
in this milestone are both lineal, the order in which the transforms
are applied is irrelevant. For this reason, we could also have used
the temporal transform inside of each channel of samples, and then,
remove the spatial redundancy.}. As it can be seen in this
\href{}{notebook}, most audio signals show ``patterns'' of samples
that tends to repeat, especially locally. Another clear source of
temporal redundancy is that the neighbor audio samples usually show
similar amplitude values.

% A decorrelating technique: DPCM
There are several techniques that can be used for removing the
temporal redundancy from a sequence of audio. One of the most
straightforward is
\href{https://en.wikipedia.org/wiki/Differential_pulse-code_modulation}{Differential
  Pulse Code Modulation
  (DPCM)~\cite{sayood2017introduction}}. However, there are more
efficient decorrelation algorithms based on
\href{https://en.wikipedia.org/wiki/Transform_coding}{transform
  coding}, such the used in the previous milestone and in this one.

% Another decorrelating technique: transform coding
Transform coding is based on the idea that we can decompose (we can
generate a decomposition from) the input signal into a set of
subbands, and if the used filters\footnote{In a more pure mathematical
  context, the
  \href{https://en.wikipedia.org/wiki/Digital_filter}{filters} are
  representing the basis vectors of a lineal transform.} are the
adecuate for removing the temporal redundancy, we can achieve a high
transform \href{https://en.wikipedia.org/wiki/Coding_gain}{coding
  gain}, accumulating most of the signal energy (and presumably most
of the information) in a small number of subbands. When this happens,
the quantization of the coefficients of the subbands will remove
basically the least significant information, allowing better
compression ratios than those in which we apply the same quantization
process to the original samples.\footnote{Notice that if we use, for
  example, a dead-zone quantizer and most of the coefficients are
  close to zero, the quantizer will generate a high number of zero
  quantization indexes and therefore, a high number of dequantized
  coefficients will be equal to zero.}

% Relation between transform coding and subband coding
The name that has been given to the previous process is
\href{https://en.wikipedia.org/wiki/Sub-band_coding}{subband
  coding}. In this context, our analysis transform matrix $K$ (see the
previous milestone) represents the coefficients (or
\href{https://en.wikipedia.org/wiki/Finite_impulse_response}{taps}) of
a 2-channels analysis
\href{https://en.wikipedia.org/wiki/Filter_bank}{Filter Bank (FB)},
and the forward transform is in fact ``descomposing'' $x$ into two subbands
$w_0$ and $w_1$ (see the Figure~\ref{fig:PRFB}, and this
\href{}{notebook}). On the other hand, the synthesis transform matrix
$K^{-1}$ denotes the taps of a synthesis FB.

% An intro to PRFBs
Let's suppose now that the filters (represented by the taps of) $K_0$
and $K_1$ are applied to the input signal $x$.\footnote{Remember,
  however, that in the context of this milestone, $x$ is a sequence of
  MST coefficients that show some degree of temporal correlation, not
  a simple pair of samples (a frame).} Let's also suppose (as happens
in the MST) that $K_0$ is a low-pass filter and $K_1$ is a high-pass
filter, and that the
\href{https://en.wikipedia.org/wiki/Filter_(signal_processing)#The_transfer_function}{transfer
  function}\footnote{The response of the filter to the
  \href{https://en.wikipedia.org/?title=Unit_impulse&redirect=no}{unit
    impulse}.} of both filters
\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{are
  the inverse of each other}. Under these assumptions, the complete
(analysis/synthesis) system is called a (2-channels)
\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{Perfect
  Reconstruction Filter Bank (PRFB)}, and $x$ can be recoverd from a
subsampled version (in this case
\href{https://en.wikipedia.org/wiki/Downsampling_(signal_processing)}{decimating}
by 2) of $w_0$ and $w_1$ (see the notebook).

% M-channels PRFB and the frequency resolution of the HAS
Using the suitable filters, it is possible to build $M$-channels
PRFBs.\footnote{Notice that our matrix $K$ would have $M$ rows in this
  case.} These filters can analyze (and synthesize) the signal $x$,
decomposing it in
(\href{https://en.wikipedia.org/wiki/Low-pass_filter#Ideal_and_real_filters}{almost
  for sure}) overlaping frequency subbands with different
bandwidth. The question here is how many filters should be used and
what bandwidth should they have. At this design point, we must
consider also that the accuracy of the humman perception of the sound
depends on the frequency: (as it can be seen in this notebook) we are
more sensitive to frequency variations when the frequency of the sound
is low. This fact is related with the existence of
\href{https://en.wikipedia.org/wiki/Bark_scale}{the bark scale} and
\href{https://en.wikipedia.org/wiki/Critical_band}{critical bands}.

% The bark scale and the DWT
The bark scale divides the audible spectrum into 24 subband of (a
priori) ``whimsical'' bandwidths. However, it's clear that a
\href{https://en.wikipedia.org/wiki/Octave_band}{dyadic partition of
  the spectrum} fits better than
\href{https://en.wikipedia.org/wiki/Wavelet_transform#Principle}{a
  lineal partition}. Considering this reason, from all the transforms
designed to date, the most suitable one from a strict frequency if the
Discrete Wavelet Transform (DWT).

% Features of the DWT
The DWT has also other interesting features:
\begin{enumerate}
\item It is
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform#Time_complexity}{fast
    ($O(N)$, where $N$ is the number of ``transformed'' elements)}.
\item Can represent efficienty
  \href{https://en.wikipedia.org/wiki/Transient_(oscillation)}{transient}
  signals, which can happen frequencly in audio.
\item Although we are not going to take advantage of the following
  characteristic (for now), one of the most interesting features of
  the DWT is that it can used to find easely a
  \href{https://en.wikipedia.org/wiki/Multiresolution_analysis}{multiresolution
    representation} of the signal.
\end{enumerate}

% Implementation alternatives for the DWT
The DWT can be implemented in different ways:
\begin{enumerate}
\item Defining $K$ and computing vector-matrix multiplications, which
  requires a calculation time proportional to $O(N^2)$. However, the
  main problem of this type of implementation is generated by the
  memory that $K$ can require, that is proportional to $N^2$.
\item
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform#Cascading_and_filter_banks}{Cascading
    PRFBs}. Considering that the convolution is a $O(N\log_2N)$
  operation, and that the number the number of levels in the cascade
  is generally small (5 for example), this implementation is faster
  than the based in vector-matrix arithmetic. And most importantly, we
  don't need to store $K$, but only the taps of the filters.
\item Using
  \href{https://en.wikipedia.org/wiki/Lifting_scheme}{lifting}, which
  provides a speed-up factor of 2. DWTs implemented with lifting do
  not need to downsample and upsample the subbands, an operation that
  is wasting the calculus of half of the coefficients at each level of
  the cascade.
\end{enumerate}

% Using the MST filters for building a DWT
In order to clarify the introduces concepts, let's build a DWT using
the MST filters and lifting.

\begin{enumerate}
\item Lifting is based on the concept of dyadic
  \href{https://en.wikipedia.org/wiki/Multiresolution_analysis}{multiresolution
    analysis} of the signals. Using it, we can rewrite the MST filter
  equations ($K_0$ and $K_1$) as
  \begin{equation}
    \begin{array}{rcl}
      l^1_i & = & x_{2i} + x_{2i+1} \\
      h^1_i & = & x_{2i+1} - x_{2i},
    \end{array}
    \label{eq:1dwt}
  \end{equation}
  where a subband $z^s=\{z_i^s|0\le i\le 2^{n-r}\}$, $2^n=N$ is the
  number of samples in $x$, and by definition, $l^0=x$, the original
  resolution level of the signal. The subbands $l^1$ and $h^1$
  computed by Eq.~\ref{rq:1dwt} are the same than the decimated
  subbands computed by a 1-levels PRFB (based on that filters), and we
  say, therefore, that Eq.~\ref{rq:1dwt} computes the 1-levels DWT.

  We define the 2-levels DWT as
  \begin{equation}
    \begin{array}{rcl}
      l^2_i & = & l^1_{2i} + l^1_{2i+1} \\
      h^2_i & = & l^1_{2i+1} - l^1_{2i},
    \end{array}
    \label{eq:2dwt}
  \end{equation}
  that, as we can see, uses as input the output of Eq.~\ref{rq:1dwt}.

  In general, for a $s$-levels DWT, we get
    \begin{equation}
    \begin{array}{rcl}
      l^s_i & = & l^{s-1}_{2i} + l^{s-1}_{2i+1} \\
      h^s_i & = & l^{s-1}_{2i+1} - l^{s-1}_{2i}.
    \end{array}
    \label{eq:2dwt}
  \end{equation}

  The $s$-levels DWT splits the signal spectrum in $s$ subbands. If
  $s=n$, we have the spectrum partition
  \begin{equation*}
    | l^s_0 | h^s_0 | h^{s-1}_0 h^{s-1}_1 | h^{s-2}_0 h^{s-2}_1 h^{s-2}_2 h^{s-2}_3 | \cdots | h^1_0 h^1_1 \cdots h^1_{2^{n-1}-1} |,
  \end{equation*}
  where it holds that
  \begin{equation}
    1+\sum_{j=1}^s 2^{j-1}=2^n.
  \end{equation}

\item Perform a number of lifting steps, each one with 2 (sub)steps:
  \begin{enumerate}
  \item Predict step: compute the $h$ subbands as a prediction error,
    that should be minimized, between the even samples (the values to
    predict) and the odd samples (the values used to predict). For the
    MST filters, we have that
    \begin{equation}
      h^s_i = l^{s-1}_{2i+1} - l^{s-1}_{2i}.
    \end{equation}
    
  \item Update step: compute the $l$ subbands considering (only) the even
    samples and the prediction errors. For the MST, we have
    \begin{equation}
      l^s_i = 2l^{s-1}_{2i} + h^s_i.
    \end{equation}
  \end{enumerate}
\end{enumerate}

Notice that these steps are invertible:
\begin{equation}
  \begin{array}{rcl}
    l^{s-1}_{2i} & = & \frac{1}{2}(l^s_i - h^s_i)\\
    l^{s-1}_{2i+1} & = & l^{s-1}_{2i} + h^s_i.
  \end{array}
\end{equation}

In the context of the wavelet theory, the response of the analysis
low-pass filter to the
\href{https://en.wikipedia.org/?title=Unit_impulse&redirect=no}{unit
  impulse}\footnote{The response of a filter to the unit impulse
  characterize the filter because the output of the filter is the set
  of taps of the filter.} is known as the scaling function and is
usually denoted by $\phi$, the response of the analysis high-pass is
known as the wavelet function and it is usually denoted by $\psi$, the
response of the synthesis low-pass filter is denoted by $\tilde\phi$
and the synthesis high-pass filter is represented by $\tilde\psi$.

For the MST it holds that $\phi\bot\psi$, that
$\tilde\phi\bot\tilde\psi$, that $\tilde\phi\bot\psi$, and that
$\phi\bot\tilde\psi$. This is true for all orthogonal DWTs. For this reason, the same 

The previous MST-based DWT is similar to other transforms such as the
Haar transform, in which we are using a 1-order predictor for removing
the temporal redundancy. Let's extend the lifting idea to a predictor
of order two. For that, we define the predict step as
\begin{equation}
  h^s_i = l^{s-1}_{2i+1} - \frac{1}{2}(l^{s-1}_{2i} + l^{s-1}_{2i+2})
\end{equation}
and the update step as
\begin{equation}
  l^s_i = 2l^{s-1}_{2i} + \frac{1}{4}(h^s_{i-1} + h^s_i),
\end{equation}
where the factor 1/4 is used for preserving the
energy~\cite{sweldens1997building}. This transform is known as the
\href{https://en.wikipedia.org/wiki/Biorthogonal_wavelet}{biorthogonal}
(2,2) of Cohen-Daubechies-Feauveau, and also as the linear transform.

Again, the linear transform in invertible by simply reversing the
steps:
\begin{equation}
  \begin{array}{rcl}
    l^{s-1}_{2i} & = & l^s_i - \frac{1}{4}(h^s_{i-1} + h^s_i)\\
    l^{s-1}_{2i+1} & = & h^s_i + \frac{1}{2}(l^{s-1}_{2i} + l^{s-1}_{2i+2}).
  \end{array}
\end{equation}

The 
  
\begin{comment}

  The Figure~\ref{fig:transform_coding} shows the stages that are
  tipycally involved in a transform-based signal compression system.

\begin{figure}
  \begin{center}
\begin{verbatim}
   s   +---+   w    +---+   k    +---+    c
 ----->| T |------->| Q |------->| E |-----------+
  (s)  +---+  (s)   +---+  (~s)  +---+   (~s)    |
samples   coefficients   indexes      code-words ~
                                                 :
                                                 ~
   ~s  +---+    w   +---+   k    +---+           |
 <-----| t |<-------| q |<-------| D |<----------+
  (~s) +---+  (~s)  +---+  (~s)  +---+
approx.    quantized     indexes
samples   coefficients
\end{verbatim}                
  \end{center}
  \caption{Common data-flow used un Transform Coding. $s$ represents
    the signal to compress, $\tilde{s}$ the lossy version of the
    reconstructed signal, $T$ the (forward) transform (which takes blocks of
    samples) producing blocks of coefficients $w$, $Q$ the scalar
    quantization stage (which takes single coefficients) producing
    quantization indexes $k$, $E$ the entropy encoder (which in our
    case (DEFLATE) works with blocks of coefficients) producing
    code-words $c$, $D$ the entropy decoder, $q$ the decuantization
    stage, and $t$ the inverse (or backward) transform. PCM stands for Puse Code
    Modulation and DEFLATE is the technique used to find a compact
    representation of the quantized coefficients.}
  \label{fig:transform_coding}
\end{figure}
\end{comment}

There exist docens of suitable transform used in TC. They are
characterized by the
\href{https://en.wikipedia.org/wiki/Orthogonality}{orthogonality}
property, which basically means that the output coefficients are going
to be decorrelated because the functions (or vectors, depending on the
terminology) that describe the transform are
\href{https://en.wikipedia.org/wiki/Orthogonal_functions}{orthogonal}
and therefore, form a
\href{https://en.wikipedia.org/wiki/Basis_(linear_algebra)}{basis
  (set)} with which is possible to describe any signal in terms
(usually as a linear combination) of the (basis) functions of such
basis, i.e., are able to describe any signal in the vector space
spanned by the basis, using an unique description (set of
coefficients)~\cite{}.

Wavelets and Wavelet Transforms
% https://cnx.org/contents/EQurkhlI@6.9:9Qg8uP5e@5/Introduction-to-Wavelets
% https://cnx.org/exports/110bab92-1948-4958-b1c7-8fc0926c392c@6.9.pdf/wavelets-and-wavelet-transforms-6.9.pdf

Other major family of transform used in TC forms bi-orthogonal basis
(sets). In this case,

In this milestone we will use an orghogonal wavelet transform. This
makes that the quantization of the coefficients can be uniform, i.e.,
we can use the same quantization step for all the coefficients.

In the transform described in the previous milestone, the concept of subband is degradated because we have only one coefficient per subband.

The coefficient $w[0]$ is called the DC (Direct Current) coefficient, and the rest of coefficients are called AC (Alternating Current) coefficients.

\href{https://en.wikipedia.org/wiki/Correlation_and_dependence}{Correlation}
is a term used in statistics which refer to the interdependency
between two \href{https://en.wikipedia.org/wiki/Random_variable}{random
  variables}. It can be measured by the
\href{https://www.mathsisfun.com/data/correlation.html}{correlation
  coefficient}~\cite{thinkstats}.

In the case of InterCom, the random variables are the two channels
(left $L$ and right $R$) of the
\href{https://en.wikipedia.org/wiki/Stereophonic_sound}{stereo
  \href{https://en.wikipedia.org/wiki/Pulse-code_modulation}{PCM}
  signal}~\cite{bosi2003intro}. In most cases, both channels are going
to be \href{https://en.wikipedia.org/wiki/Binaural_recording}{highly
  correlated} (especially if the microphone is mono), which means that
we can represent one of them (for example, the $R$ channel) with
respect to the other (the $L$ channel). From a mathematical point of
view, this process can be seen as a
\href{https://en.wikipedia.org/wiki/Decorrelation}{decorrelation}
process. From a physical perspective, decorrelating implies energy
accumulation in a few coefficients~\cite{sayood2017introduction}.

To perform this inter-channel decorrelation, we can use an
\href{https://en.wikipedia.org/wiki/Orthogonal_transformation}{orthogonal}\footnote{Orthogonality
  of the transform is a important property because the correlation
  between the coefficients (the output of the transform) is 0.}
transform, that in the case of decorrelating a stereo signal is
\begin{equation}
  y = Kx = \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}x,
\end{equation}
where $x$ represents a frame (a tuple of L and R samples, $x[0]$ and
$x[1]$), $K$ is the $2\times 2$ KLT
\href{http://fourier.eng.hmc.edu/e161/lectures/klt/node3.html}{(Karhunen-Lo\`eve
  Transform)} matrix multiplied by $1/\sqrt{2}$ (which is closely
related to the \href{http://wavelets.pybytes.com/wavelet/haar/}{Haar
  transform}~\cite{vetterli1995wavelets}), and $y$ represents the
transform coefficients (in our case, a couple of coefficients $y[0]$
with the \href{https://en.wikipedia.org/wiki/Arithmetic_mean}{mean}
and $y[1]$ with the difference of the samples). Notice that this
transform is not
\href{https://en.wikipedia.org/wiki/Orthonormal_basis}{orthonormal}
(energy preserving in the transform domain) because
\begin{equation}
  \sum y[i]^2 = \frac{1}{\sqrt{2}}\sum x[i]^2,
\end{equation}
although both subbands $y[0]$ and $y[1]$ have the same gain
($\frac{1}{\sqrt{2}}$, and therefore the same ``importance'' for a
future
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization}
of $y$). The described transform is similar to the so called
\href{https://en.wikipedia.org/wiki/Joint_encoding#M/S_stereo_coding}{M/S
  stereo coding}, but in our case, the divisi√≥n by 2 is carried on
the forward transform, instead of the backward (inverse) transform.

This transform can be implemented
\href{https://en.wikipedia.org/wiki/In-place_algorithm}{\emph{in-place}}
using the following algorithm:

\begin{pseudocode}{Inter-channel\_decorrelation}{~}
  \PROCEDURE{analyze}{\text{frame}}
  \BEGIN
    \text{frame}[0] -= \text{frame}[1] \\
    \text{frame}[1] += (\text{frame}[0] / 2) \\
    \text{frame}[0] /= 2
  \END
  \ENDPROCEDURE
  \PROCEDURE{synthesize}{\text{frame}}
  \BEGIN
    \text{frame}[0] *= 2 \\
    \text{frame}[1] -= (\text{frame}[0] / 2) \\
    \text{frame}[0] += \text{frame}[1]
  \END
  \ENDPROCEDURE
\end{pseudocode}

where $\text{a}~\mathtt{OPER}= \text{b}$ is a shorter representation of the operation
$\text{a} = \text{a}~\mathtt{OPER}~\text{b}$. Notice that this type of in-place computations
are commonly used in the implementation of DWTs
(\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{Dicrete
  Wavelet Transform}s) using
\href{https://cm-bell-labs.github.io/who/wim/papers/athome/athome.pdf}{the
  Lifting Scheme}~\cite{2006.sweldens}.

\section{What you have to do?}

\begin{enumerate}
\item In a module named stereo.py, inherit the class
  Quantization and create a class named Stereo\_decorrelation.
\item Override the methods pack() and unpack(). In
  pack() perform the procedure analyze() previously
  described, and in unpack() the
  synthesize(). These procedures should be applied to
  all the frames of a chunk using \href{https://www.oreilly.com/library/view/python-for-data/9781449323592/ch04.html}{vectorized
    operations}.
\item Has been the
  \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio} improved (on
  \href{https://en.wikipedia.org/wiki/Average}{average})? How much?
\end{enumerate}

\section{Timming}

You should reach this milestone at most one week.

\section{Deliverables}

The module stereo.py. Store it at the
\href{https://github.com/Tecnologias-multimedia/intercom}{root
  directory} of your InterCom's repo.

\section{Resources}

\bibliography{maths,data-compression,DWT,audio-coding}
