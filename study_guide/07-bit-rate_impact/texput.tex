\input{../definitions}
\title{\TM{} - Study Guide - Milestone 7: Impact of the Bit-rate}

\maketitle

\section{Description}

Along with the latency and its variation (jitter), another main aspect
to consider about the
\href{https://en.wikipedia.org/wiki/Telecommunications_link}{transmission
  link} used in an InterCom session is the
\href{https://en.wikipedia.org/wiki/Bit_rate}{transmission bit-rate}
(measured in bits per second or a $10$-multiple of this transmission
capacity) that it can provide~\cite{Forouzan,Tanenbaum}. This bit-rate
depend on the maximum
\href{https://en.wikipedia.org/wiki/Bandwidth_(computing)}{capacity}
(a characteristic closely related with the available
\href{https://en.wikipedia.org/wiki/Bandwidth_(signal_processing)}{bandwidth})
and the
\href{https://en.wikipedia.org/wiki/Network_congestion}{congestion
  level} (that basically depends on the load) of the link. In general,
we can suppose that the capacity is constant along the time (the
bandwidth provided by the link does not vary with the time). On the
contrary, the congestion level is time-variying and quite
unpredictable, because it depends on the behaviour of the network
users.

In this milestone we are going to measure the impact of the
transmission bit-rate on the
\href{https://en.wikipedia.org/wiki/Quality_of_experience}{QoE}\footnote{Follow
  the same rubric than in the case of the measurement of the impact of
  the latency.} provided by the minimal implementation of InterCom. Similarly to the procedure used for measuring the impact of
the latency and the jitter, we will use
\href{https://man7.org/linux/man-pages/man8/tc.8.html}{\texttt{tc}}~\cite{bert2012lartc}
to control the amount of data that an InterCom instance will be
allowed to send in a local environment, with the aim of simulating a
real running environment. Notice that this upper bound in the bit-rate
will also affect to the loss of chunks because if the transmission
bit-rate is smaller than the audio bit-rate, sooner of later the link
will discard those chunks that can not be buffered in the retransmission
nodes (routers and switches)\footnote{Notice that, in this case, we
would be at least contributing, if not causing, the link congestion.}.

\section{What you have to do?}

\subsection{Estimate the bit-rate in an Internet link}

Usually, we need to use a tool such as \href{https://iperf.fr/}{iPerf}
to
\href{https://en.wikipedia.org/wiki/Measuring_network_throughput}{measure
  the transmission bit-rate} between
two \href{https://datatracker.ietf.org/doc/html/rfc4113}{end-points}
(of
different \href{https://en.wikipedia.org/wiki/Host_(network)}{hosts})
in the Internet. This proceduce implies that we must have access to
both hosts to install and run this program, privileges that are not
always available.

Alternatively, we can estimate the
\href{https://en.wikipedia.org/wiki/Throughput}{link throughput} using
\href{https://github.com/torvalds/linux/blob/master/net/ipv4/ping.c}{\texttt{ping}}
(see the previous milestone). At this point is important to realize
that \texttt{ping} has been designed to measure latencies, not
bit-rates, and that for this reason, we will be able only to estimate
throughputs, rougtly.

The transmission bitrate available between two directly reachable IP
devices can be estimated as
\begin{equation}
  b=\frac{B}{t_t},
  \label{eq:b}
\end{equation}
where $B$ is the number of bits sent in a \verb|ping|
\href{https://en.wikipedia.org/wiki/Payload_(computing)}{payload}, and
\begin{equation}
  t_t = \frac{\text{RTT}-2t_p}{2},
  \label{eq:tt}
\end{equation}
an equation that can be determined by revisiting the \verb|ping|
timeline (Fig.~1) and the Eq.~2 of the
\href{https://tecnologias-multimedia.github.io/study_guide/06-jitter_impact/}{previous
  milestone}.

Notice that if the payload of the ping is small enough ($0$ bytes
ideally, and therefore $t_t=0$), we can consider that
\begin{equation}
  t_p \approx \frac{\text{RTT}}{2},
  \label{eq:tp}
\end{equation}
and that for measuring the $\text{RTT}$ in
Eq.~\ref{eq:tt}, we should use a payload as large as possible
(ideally, infinite bytes), in order to make $t_t$ a significative
amount of time, ease to measure. Notice also that we should use the
smaller RTT value of a \texttt{ping} session of measurements in the
Eq.~\ref{eq:tp} because the minimum link latency is a constant and the
congestion of the link always increases it.

Therefore, supposing that $t_p$ is constant and using the
average\footnote{Remember that with the \texttt{ping} command we
iterate several times and the tool returns average values.}
$\text{RTT}$ for transmitting $B$ bits ($B$ as
large as possible), the bit-rate value $b$ determined by
Eq.~\ref{eq:b} can be considered as a good estimation for the link of
the near future.

So, for determining $b$ we must run the following steps:
\begin{enumerate}
\item Run \texttt{ping} using the smallest possible payload that
  provides RTTs values.\footnote{The minimal payload for a
    \texttt{ping} message can be 0, but depending on the
    implementation of \texttt{ping} you may be forzed to use a larger
    value.} Use it for computing $t_p$.
  
\item Run \texttt{ping} using the largest possible payload to achieve
  that the most part of the average RTT provided by \texttt{ping}
  corresponds to $t_t$. It is necessary to use the highest
  payload-size because the accuracy of the cronometer used by
  \texttt{ping}, in general, is not high enough.
  
\item Use Eq.~\ref{eq:tt} to find $t_t$ as a function of the average
  RTT and the minimal $t_p$.
  
\item Finally, use Eq.~\ref{eq:b} to determine $b$.
\end{enumerate}

If the devices cannot \texttt{ping} each other, and supposing that
there is only one point in the communication link that is filtering
the ICMP Echo Request traffic in each direction, then we can apply the
same technique that was used in the previous milestone, which
basically consists in computing the total RTT as the sum of the RTTs
from the devices that we want to connect (where the InterCom is it
supposed to be run) to the filter (possiblely, one of your home
routers). A similar capacity estimation should be obtained considering
only the slower part of the total link.

(Optional) In order to gain experience with all this stuff, measure
your bandwith (using your local host) with different \texttt{ping}
servers, try to compute the transmission bit-rate:
\begin{enumerate}
\item In \href{https://en.wikipedia.org/wiki/Localhost}{\texttt{localhost}}.
\item In \texttt{localhost}, limiting the bit-rate with \texttt{tc}.
\item Between two hosts in your
  \href{https://en.wikipedia.org/wiki/Local_area_network}{local
    network} (your host and your router, for example). Notice that in
  this case, you should not obtain a bit-rate higher than the capacity
  of your
  \href{https://en.wikipedia.org/wiki/Network_interface_controller}{network
    adapter}.
\item Between your host and a public host of the Internet. Try
  \href{https://www.meter.net/}{\texttt{www.meter.net}} for example.
\end{enumerate}

\subsection{Simulate the link}

With the bit-rate value $b$ computed from the previous section, simulate
such link (locally) with:

\begin{lstlisting}{language=bash}
  sudo tc qdisc add dev lo root handle 1: tbf rate <bit-rate_in_kbps>kbit burst 32kbit limit 32kbit
  sudo tc qdisc add dev lo root parent 1:1 handle 10: netem delay <average_delay_in_miliseconds>ms <maximum_average_deviation_in_miliseconds>ms <Pearson_correlation_coefficient_expressed_as_a_percentage>% distribution <uniform|normal|pareto|paretonormal>
\end{lstlisting}

Examples:

\begin{enumerate}
\item Check the current configuration:
  
  \begin{lstlisting}{language=bash}
    tc qdisc show dev lo
  \end{lstlisting}
  
  The output should be something like:
  
  \begin{lstlisting}{language=bash}
    qdisc noqueue 0: root refcnt 2
  \end{lstlisting}
  
\item Define the rule for the bit-rate control (example for $200$ kbps):
  
  \begin{lstlisting}{language=bash}
    sudo tc qdisc add dev lo root handle 1: tbf rate 200kbit burst 32kbit limit 32kbit
  \end{lstlisting}
  
\item Define the rule for the latency control (see previous milestone)
  (example for $100$ ms, $10$ ms of jitter, and
  \href{https://wiki.linuxfoundation.org/networking/netem}{with the
    next random transmission depending $25$\% on the last one}):
  
  \begin{lstlisting}{language=bash}
    sudo tc qdisc add dev lo parent 1:1 handle 10: netem delay 100ms 10ms 25% distribution normal
  \end{lstlisting}
  
\item After adding these rules, this should be the configuration:
  
  \begin{lstlisting}{language=bash}
    tc qdisc show dev lo
  \end{lstlisting}
  
  The output should be something like:
  
  \begin{lstlisting}{language=bash}
    qdisc tbf 1: root refcnt 2 rate 200Kbit burst 4Kb lat 0us 
    qdisc netem 10: parent 1:1 limit 1000 delay 100ms  10ms 25%
  \end{lstlisting}

\end{enumerate}

Remember to delete the rules after their are unnecessary (example to delete the previous rules):

\begin{lstlisting}{language=bash}
  sudo tc qdisc delete dev lo parent 1:1 handle 10: netem delay 100ms 10ms 25% distribution normal
  sudo tc qdisc delete dev lo root handle 1: tbf rate 200kbit burst 32kbit limit 32kbit
\end{lstlisting}

\section{Timming}

Please, finish this milestone in one week.

\section{Deliverables}

A report showing your results.

\section{Resources}

\bibliography{networking}
