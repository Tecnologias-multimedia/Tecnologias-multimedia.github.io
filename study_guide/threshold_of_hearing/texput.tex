% Emacs, this is -*-latex-*-

\title{Perceptual Coding Considering the Threshold of Hearing}

\maketitle

\section{Description}

\subsection{A model of the Threshold of Human Hearing}

Psychoacoustics (see
\href{https://vicente-gonzalez-ruiz.github.io/the_sound/}{the sound},
\href{https://vicente-gonzalez-ruiz.github.io/human_auditory_system/}{the
  human auditory system}, and
\href{https://vicente-gonzalez-ruiz.github.io/human_sound_perception/}{the
  human sound perception}) has determined that the HAS (Human Auditory
System) has a sensitivity that depends on the frequency of the sound,
the so called THH
(\href{https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing}{Threshold
  of Human Hearing}). This basically means that some subbands can be
quantized with a larger quantization step than others without a
noticeable increase (from a perfection perspective) of the
quantization noise.

\begin{figure}
  \centering
  %\svg{graphics/ToHH}{1800}
  \myfig{graphics/ToHH}{47cm}{1400}
  \caption{A model for the threshold of human hearing.}
  \label{fig:ToHH}
\end{figure}

A good approximation of the THH for a 20-years old person can be
obtained with~\cite{bosi2003intro}
\begin{equation}
  T(f)\text{[dB]} = 3.64(f\text{[kHz]})^{-0.8} - 6.5e^{f\text{[kHz]}-3.3)^2} + 10^{-3}(f\text{[kHz]})^4.
  \label{eq:ToHH}
\end{equation}
This equation has been plotted in the Fig.~\ref{fig:ToHH}.

\subsection{DWT subbands and quantization steps}
The number of DWT subbands
\begin{equation}
  N_{\text{sb}} = N_{\text{levels}} + 1
\end{equation}
where $N_{\text{levels}}$ is the number of levels of the DWT. Except
for the ${\mathbf l}^{N_{\text{levels}}}$ subband (the lowest-pass
frequency of the decomposition), it holds that
\begin{equation}
  W({\mathbf h}^s) = \frac{1}{2}W({\mathbf h}^{s-1}),
\end{equation}
being $W(\cdot)$ the bandwidth of the corresponding
subband $s$. Therefore, considering that the bandwidth of the audio signal
is $22050$ Hz, the bandwidth $W({\mathbf h}^1)$ of the ${\mathbf h}^1$ subband is $11025$ Hz,
$W({\mathbf h} ^2)=22025/4$, and so on. It also holds that
\begin{equation}
  W({\mathbf l}^{N_{\text{levels}}}) = W({\mathbf h}^{N_{\text{levels}}}).
\end{equation}

The idea is to decide, knowing the frequencies represented in each DWT
subband and the THH curve, the quantization step that should be
applied to each subband. This idea has been implemented in the module
\verb|threshold.py|.

\section{What you have to do?}

\subsection{Determine your threshold of hearing}

The threshold of hearing plotted in the Fig.~\ref{fig:ToHH} can be
different to the curve which corresponds with your current ``hearing
capabilities''.\footnote{For example, your speakers could not have a
  flat frequency response, or your room could attenuate more, some
  freqencies.} For this reason, in the module \verb|threshold.py|,
implement a procedure for determining the quantization step sizes used in
each Wavelet subband\footnote{Remember that when the
  \href{https://en.wikipedia.org/wiki/Wavelet_transform}{Wavalet
    transform} is
  \href{https://en.wikipedia.org/wiki/Dyadic_rational}{dyadic}, the
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{Wavelet
    space} is analyzed by
  \href{https://en.wikipedia.org/wiki/Octave_band}{octaves}, and
  therefore the
  \href{https://en.wikipedia.org/wiki/Filter_bank}{subbands} doubles
  the size when we increase the frequency.}.
  
To find out such quantization step sizes, we can simulate
\href{https://en.wikipedia.org/wiki/Quantization_(signal_processing)}{quantization
  noise} in each subband and measure the amplitude of the noise when
it starts to be noticeable. Supposing that the quantization noise
follows an
\href{https://en.wikipedia.org/wiki/Continuous_uniform_distribution}{uniform
  distribution}, the next algorithm can de implemented:

\begin{enumerate}
\item Let
  $\{{\mathbf l}^{N_{\text{levels}}}, {\mathbf h}^{N_{\text{levels}}},
  {\mathbf h}^{N_{\text{levels}}-1},\cdots, {\mathbf h}^1\}$ the
  Wavelet representation of a
  chunk. %, being ${\mathbf l}^{N_{\text{levels}}}$ the lowest frequency subband.
  Starting with ${\mathbf l}^{N_{\text{levels}}}$ (at the first
  iteration the rest of subbands are zero), while the noise stays
  imperceptible:
  \begin{enumerate}
  \item Increase the amplitude of the noise in the subband.
  \item Compute the inverse Wavelet transform of $\{{\mathbf
    l}^{N_{\text{levels}}}, {\mathbf h}^{N_{\text{levels}}}, {\mathbf
    h}^{N_{\text{levels}}-1},\cdots, {\mathbf h}^1\}$.
  \item Reproduce the generated data, alternating every second with
    the play of an empty chunk.
  \end{enumerate}
  \item Continue with the next subband, but keeping the lowest
    perceptible uniform noise in the previously processed ones. In
    this way, for the subband ${\mathbf h}^1$ (the last one to be
    processed), the rest of subbands should have already stored noise.
\end{enumerate}

Notice that the quantization step sizes are determined for the sound
that you are going to play. Therefore, you should use your
interlocutor's quantization step sizes and viceversa.

Finally, this extra functionality should be optional for the user of
InterCom (the ``standard'' ToHH, see Fig.~\ref{fig:ToHH}, should be
used by default).

\subsection{Subjective performance}

\begin{enumerate}
\item Using a recording tool such as
  \href{http://audacity.sourceforge.net}{Audacity} or
  \href{http://plugin.org.uk/timemachine/}{JACK Timemachine}, record
  the simulated transmission of a piece of audio and create a
  \texttt{.wav} file, when the audio has been transmitted using
  \texttt{temporal\_overlapped\_DWT\_coding.py} and
  \texttt{threshold.py}, using in both cases the same transmission
  bit-rate. Vary the quantization step size for controlling the
  bit-rate.
\item Determine which audio sounds better, from a subjective point of
  view. Repeat this step the number of times you consider necessary.
\end{enumerate}

\section{Deliverables}

The module threshold.py and a report of how your proposal works,
including a subjective performance comparison.

\section{Resources}

\bibliography{maths,data-compression,DWT,audio-coding}

